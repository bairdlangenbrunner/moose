{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as mp\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.cm as cm\n",
    "import mpl_toolkits.mplot3d\n",
    "import matplotlib\n",
    "import scipy.ndimage\n",
    "import datetime\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import numpy.random\n",
    "import scipy.stats\n",
    "import os\n",
    "\n",
    "mp.rcParams.update({'mathtext.default': 'regular'})\n",
    "\n",
    "from mpl_toolkits import basemap\n",
    "import mpl_toolkits.axes_grid1\n",
    "\n",
    "degree_sign = u'\\u00B0'\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = numpy.array(( \\\n",
    "'ACCESS1-0', \\\n",
    "'ACCESS1-3', \\\n",
    "'bcc-csm1-1-m', \\\n",
    "'bcc-csm1-1', \\\n",
    "'BNU-ESM', \\\n",
    "'CanESM2', \\\n",
    "'CCSM4', \\\n",
    "'CESM1-BGC', \\\n",
    "'CESM1-CAM5', \\\n",
    "'CMCC-CESM', \\\n",
    "'CMCC-CM', \\\n",
    "'CMCC-CMS', \\\n",
    "'CNRM-CM5', \\\n",
    "'CSIRO-Mk3-6-0', \\\n",
    "'EC-EARTH', \\\n",
    "'FGOALS-g2', \\\n",
    "'GFDL-CM3', \\\n",
    "'GFDL-ESM2G', \\\n",
    "'GFDL-ESM2M', \\\n",
    "'GISS-E2-H', \\\n",
    "'GISS-E2-R', \\\n",
    "'HadGEM2-AO', \\\n",
    "'HadGEM2-CC', \\\n",
    "'HadGEM2-ES', \\\n",
    "'inmcm4', \\\n",
    "'IPSL-CM5A-LR', \\\n",
    "'IPSL-CM5A-MR', \\\n",
    "'IPSL-CM5B-LR', \\\n",
    "'MIROC5', \\\n",
    "'MIROC-ESM-CHEM', \\\n",
    "'MIROC-ESM', \\\n",
    "'MPI-ESM-LR', \\\n",
    "'MPI-ESM-MR', \\\n",
    "'MRI-CGCM3', \\\n",
    "'NorESM1-ME', \\\n",
    "'NorESM1-M' ))\n",
    "nmods = len(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_names = [\"djf\",\"mam\",\"jja\",\"son\",\"annual\",\"ondjfm\"]\n",
    "\n",
    "pr_lat_lo, pr_lat_hi, pr_lon_lo, pr_lon_hi = 30., 45., 232.5, 248; region = 'CA'\n",
    "\n",
    "ts_lat_lo, ts_lat_hi, ts_lon_lo, ts_lon_hi = -30., 10., 155., 270.; region = 'tropacific'\n",
    "\n",
    "ua_lat_lo, ua_lat_hi, ua_lon_lo, ua_lon_hi = 20., 50., 170., 250.; region = 'midlatpacific'\n",
    "\n",
    "tel_lat_lo, tel_lat_hi, tel_lon_lo, tel_lon_hi = 25., 60., 220., 255.; region = 'CA'\n",
    "\n",
    "season='djf'; SEASON='DJF'\n",
    "#season='jja'; SEASON='JJA'\n",
    "#season='annual'; SEASON='annual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPEN TS DATASET\n",
    "ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/ts/djf/NorESM1-M_ts_1970-00_climatology_djf.nc', 'r', format='NETCDF4')\n",
    "\n",
    "ts_data_orig = ncfile.variables['ts'][:]\n",
    "ts_lat = ncfile.variables['lat'][:]\n",
    "ts_lon = ncfile.variables['lon'][:]\n",
    "\n",
    "# pull out lat/lon indices\n",
    "ts_lat_inds = numpy.where((ts_lat>=ts_lat_lo) & (ts_lat<=ts_lat_hi))[0]\n",
    "ts_lon_inds = numpy.where((ts_lon>=ts_lon_lo) & (ts_lon<=ts_lon_hi))[0]\n",
    "ts_regional_lat_vals = ts_lat[ts_lat_inds[0]:(ts_lat_inds[-1]+1)]\n",
    "ts_regional_lon_vals = ts_lon[ts_lon_inds[0]:(ts_lon_inds[-1]+1)]    \n",
    "\n",
    "ts_data = ts_data_orig[ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "ts_regional_nlat, ts_regional_nlon = ts_data.shape\n",
    "global_nlat, global_nlon = ts_data_orig.shape[0:2]\n",
    "global_lat_vals = ts_lat[:]\n",
    "global_lon_vals = ts_lon[:]\n",
    "\n",
    "## CALCULATE WEIGHTS BY LATITUDE\n",
    "#weights_1d = numpy.cos((numpy.pi/180.)*regional_lat_vals)\n",
    "## THEN TILE THEM TOGETHER TO FILL A 72x144 ARRAY\n",
    "#weights_2d = numpy.tile(weights_1d, reps=(regional_lon_vals.shape[0],1)).T\n",
    "#weights_2d_rvld = numpy.reshape(weights_2d, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPEN TS OBSERVATIONS\n",
    "ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/obs_calculations/ppe_obs_comparisons/obs_era_interim_72x144_skt_1970-2000_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "obs_field_ts = ncfile.variables['skt'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "\n",
    "# OPEN PR OBSERVATIONS\n",
    "ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/obs_calculations/pr_gpcp/obs_GPCP_72x144_PRECT_1970-2000_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "pr_lat = ncfile.variables['lat'][:]\n",
    "pr_lon = ncfile.variables['lon'][:]\n",
    "pr_lat_inds = numpy.where((pr_lat>=pr_lat_lo) & (pr_lat<=pr_lat_hi))[0]\n",
    "pr_lon_inds = numpy.where((pr_lon>=pr_lon_lo) & (pr_lon<=pr_lon_hi))[0]\n",
    "obs_field_pr = ncfile.variables['PRECT'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "pr_regional_nlat, pr_regional_nlon = obs_field_pr.shape\n",
    "\n",
    "pr_regional_lat_vals = pr_lat[pr_lat_inds[0]:(pr_lat_inds[-1]+1)]\n",
    "pr_regional_lon_vals = pr_lon[pr_lon_inds[0]:(pr_lon_inds[-1]+1)] \n",
    "\n",
    "# OPEN UA OBSERVATIONS\n",
    "ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/obs_calculations/u200_MERRA/obs_MERRA_2.5x2.5_u200_1970-00_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "ua_lat = ncfile.variables['lat'][:]\n",
    "ua_lon = ncfile.variables['lon'][:]\n",
    "ua_lat_inds = numpy.where((ua_lat>=ua_lat_lo) & (ua_lat<=ua_lat_hi))[0]\n",
    "ua_lon_inds = numpy.where((ua_lon>=ua_lon_lo) & (ua_lon<=ua_lon_hi))[0]\n",
    "obs_field_ua = ncfile.variables['u200'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]\n",
    "ua_regional_nlat, ua_regional_nlon = obs_field_ua.shape\n",
    "\n",
    "ua_regional_lat_vals = ua_lat[ua_lat_inds[0]:(ua_lat_inds[-1]+1)]\n",
    "ua_regional_lon_vals = ua_lon[ua_lon_inds[0]:(ua_lon_inds[-1]+1)] \n",
    "\n",
    "# OPEN PR TELECONNECTIONS OBSERVATIONS\n",
    "ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/obs_calculations/pr_teleconnections/pr_teleconnections_GPCP_ERSSTv4_nino34_1979-2010_djf.nc', 'r', format='NETCDF4')\n",
    "tel_lat = ncfile.variables['lat'][:]\n",
    "tel_lon = ncfile.variables['lon'][:]\n",
    "obs_field_tel = ncfile.variables['pr'][(tel_lat>=tel_lat_lo)&(tel_lat<=tel_lat_hi),:][:,(tel_lon>=tel_lon_lo)&(tel_lon<=tel_lon_hi)]\n",
    "tel_regional_lat_vals = tel_lat[(tel_lat>=tel_lat_lo)&(tel_lat<=tel_lat_hi)]\n",
    "tel_regional_lon_vals = tel_lon[(tel_lon>=tel_lon_lo)&(tel_lon<=tel_lon_hi)]\n",
    "tel_regional_nlat, tel_regional_nlon = obs_field_tel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up data\n",
    "model_data_hist_pr = numpy.zeros((len(model_names), pr_regional_nlat, pr_regional_nlon))\n",
    "model_data_eoc_pr = numpy.zeros((len(model_names), pr_regional_nlat, pr_regional_nlon))\n",
    "model_data_hist_pr_LENS = numpy.zeros((40, pr_regional_nlat, pr_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/pr/'+season+'/'+modelname+'_pr_1970-00_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_pr[i,:,:] = ncfile.variables['pr'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "    # OPEN FUTURE CHANGE FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/pr/'+season+'/'+modelname+'_pr_2070-99_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_eoc_pr[i,:,:] = ncfile.variables['pr'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "\n",
    "LENS_names = ['{:02d}'.format(i) for i in range(1,36)] + ['{:03d}'.format(i) for i in range(101,106)]\n",
    "for i in range(len(LENS_names)): # 40\n",
    "    member_name = LENS_names[i]\n",
    "    # get convective precipitation\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cesm_LENS_calculations/PRECC/'+season+'/'+member_name + '_PRECC_1970-00_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    precc_temp = ncfile.variables['PRECC'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    # get large-scale precipitation\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cesm_LENS_calculations/PRECL/'+season+'/'+member_name + '_PRECL_1970-00_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    precl_temp = ncfile.variables['PRECL'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    # add together\n",
    "    model_data_hist_pr_LENS[i,:,:] = precc_temp + precl_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT TS DATA\n",
    "model_data_hist_ts = numpy.zeros((len(model_names), ts_regional_nlat, ts_regional_nlon))\n",
    "model_data_eoc_ts = numpy.zeros((len(model_names), ts_regional_nlat, ts_regional_nlon))\n",
    "model_data_hist_ts_LENS = numpy.zeros((40, ts_regional_nlat, ts_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/ts/'+season+'/'+modelname+'_ts_1970-00_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ts[i,:,:] = ncfile.variables['ts'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "    # OPEN FUTURE CHANGE FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/ts/'+season+'/'+modelname+'_ts_2070-99_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_eoc_ts[i,:,:] = ncfile.variables['ts'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "\n",
    "LENS_names = ['{:02d}'.format(i) for i in range(1,36)] + ['{:03d}'.format(i) for i in range(101,106)]\n",
    "for i in range(len(LENS_names)): # 40\n",
    "    member_name = LENS_names[i]\n",
    "    # get convective precipitation\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cesm_LENS_calculations/TS/'+season+'/'+member_name + '_TS_1970-00_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ts_LENS[i,:,:] = ncfile.variables['TS'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT UA200 DATA\n",
    "model_data_hist_ua = numpy.zeros((len(model_names), ua_regional_nlat, ua_regional_nlon))\n",
    "model_data_eoc_ua = numpy.zeros((len(model_names), ua_regional_nlat, ua_regional_nlon))\n",
    "model_data_hist_ua_LENS = numpy.zeros((40, ua_regional_nlat, ua_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/ua200/'+season+'/'+modelname+'_ua200_1970-00_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ua[i,:,:] = ncfile.variables['ua'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "    # OPEN FUTURE CHANGE FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/ua200/'+season+'/'+modelname+'_ua200_2070-99_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_eoc_ua[i,:,:] = ncfile.variables['ua'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "\n",
    "LENS_names = ['{:02d}'.format(i) for i in range(1,36)] + ['{:03d}'.format(i) for i in range(101,106)]\n",
    "for i in range(len(LENS_names)): # 40\n",
    "    member_name = LENS_names[i]\n",
    "    # get convective precipitation\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cesm_LENS_calculations/U200/'+season+'/'+member_name + '_U_1970-00_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ua_LENS[i,:,:] = ncfile.variables['U'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT TELECONNECTIONS DATA\n",
    "model_data_hist_tel = numpy.zeros((len(model_names), tel_regional_nlat, tel_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('/Users/baird/Dropbox/_data_analyzed/cmip5_calculations/pr_teleconnections/'+modelname+'_pr_teleconnections_nino34_1961-90_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_tel[i,:,:] = ncfile.variables['pr'][(tel_lat>=tel_lat_lo)&(tel_lat<=tel_lat_hi),:][:,(tel_lon>=tel_lon_lo)&(tel_lon<=tel_lon_hi)]\n",
    "    ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncal_latlon = numpy.load('/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/ncal_latlon_array.npy')\n",
    "ccal_latlon = numpy.load('/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/ccal_latlon_array.npy')\n",
    "scal_latlon = numpy.load('/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/scal_latlon_array.npy')\n",
    "coastal_cal_latlon = numpy.load('/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/coastal_cal_latlon_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# precip indices\n",
    "pr_indices_lon_ncal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, ncal_latlon[i,0]))[0][0] for i in range(ncal_latlon.shape[0]) ]\n",
    "pr_indices_lat_ncal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, ncal_latlon[i,1]))[0][0] for i in range(ncal_latlon.shape[0]) ]\n",
    "\n",
    "pr_indices_lon_ccal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, ccal_latlon[i,0]))[0][0] for i in range(ccal_latlon.shape[0]) ]\n",
    "pr_indices_lat_ccal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, ccal_latlon[i,1]))[0][0] for i in range(ccal_latlon.shape[0]) ]\n",
    "\n",
    "pr_indices_lon_scal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, scal_latlon[i,0]))[0][0] for i in range(scal_latlon.shape[0]) ]\n",
    "pr_indices_lat_scal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, scal_latlon[i,1]))[0][0] for i in range(scal_latlon.shape[0]) ]\n",
    "\n",
    "pr_indices_lon_coastal_cal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, coastal_cal_latlon[i,0]))[0][0] for i in range(coastal_cal_latlon.shape[0]) ]\n",
    "pr_indices_lat_coastal_cal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, coastal_cal_latlon[i,1]))[0][0] for i in range(coastal_cal_latlon.shape[0]) ]\n",
    "\n",
    "# not sure if I'll need teleconnection indices but here they are in case...\n",
    "tel_indices_lon_ncal = [ numpy.where(numpy.in1d(tel_regional_lon_vals, ncal_latlon[i,0]))[0][0] for i in range(ncal_latlon.shape[0]) ]\n",
    "tel_indices_lat_ncal = [ numpy.where(numpy.in1d(tel_regional_lat_vals, ncal_latlon[i,1]))[0][0] for i in range(ncal_latlon.shape[0]) ]\n",
    "\n",
    "tel_indices_lon_ccal = [ numpy.where(numpy.in1d(tel_regional_lon_vals, ccal_latlon[i,0]))[0][0] for i in range(ccal_latlon.shape[0]) ]\n",
    "tel_indices_lat_ccal = [ numpy.where(numpy.in1d(tel_regional_lat_vals, ccal_latlon[i,1]))[0][0] for i in range(ccal_latlon.shape[0]) ]\n",
    "\n",
    "tel_indices_lon_scal = [ numpy.where(numpy.in1d(tel_regional_lon_vals, scal_latlon[i,0]))[0][0] for i in range(scal_latlon.shape[0]) ]\n",
    "tel_indices_lat_scal = [ numpy.where(numpy.in1d(tel_regional_lat_vals, scal_latlon[i,1]))[0][0] for i in range(scal_latlon.shape[0]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take all data and ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW TAKE ALL PR DATA AND RAVEL IT\n",
    "# CALCULATE ENSEMBLE MEAN FOR EOC CONVERGENCE\n",
    "model_field_mmem_pr = numpy.mean(model_data_eoc_pr, axis=0)\n",
    "model_field_mmem_ts = numpy.mean(model_data_eoc_ts, axis=0)\n",
    "model_field_mmem_ua = numpy.mean(model_data_eoc_ua, axis=0)\n",
    "#model_field_mmem_tel = numpy.mean(model_data_hist_tel, axis=0)\n",
    "\n",
    "# NOW CALCULATE BIAS AND CONVERGENCE\n",
    "bias_values_pr = numpy.zeros((nmods))\n",
    "convergence_values_pr = numpy.zeros((nmods))\n",
    "\n",
    "bias_values_ts = numpy.zeros((nmods))\n",
    "convergence_values_ts = numpy.zeros((nmods))\n",
    "\n",
    "bias_values_ua = numpy.zeros((nmods))\n",
    "convergence_values_ua = numpy.zeros((nmods))\n",
    "\n",
    "bias_values_tel = numpy.zeros((nmods))\n",
    "correlation_vals_tel = numpy.zeros((nmods))\n",
    "\n",
    "for i in range(nmods):\n",
    "    hist_field_pr = model_data_hist_pr[i,:,:]\n",
    "    eoc_field_pr = model_data_eoc_pr[i,:,:]\n",
    "    \n",
    "    hist_field_ts = model_data_hist_ts[i,:,:]\n",
    "    eoc_field_ts = model_data_eoc_ts[i,:,:]\n",
    "\n",
    "    hist_field_ua = model_data_hist_ua[i,:,:]\n",
    "    eoc_field_ua = model_data_eoc_ua[i,:,:]\n",
    "\n",
    "    hist_field_tel = model_data_hist_tel[i,:,:]\n",
    "\n",
    "    bias_values_pr[i] = numpy.sqrt( numpy.mean((hist_field_pr - obs_field_pr)**2.) )\n",
    "    convergence_values_pr[i] = numpy.sqrt( numpy.mean((eoc_field_pr - model_field_mmem_pr)**2.) )\n",
    "    \n",
    "    bias_values_ts[i] = numpy.sqrt( numpy.mean((hist_field_ts - obs_field_ts)**2.) )\n",
    "    convergence_values_ts[i] = numpy.sqrt( numpy.mean((eoc_field_ts - model_field_mmem_ts)**2.) )\n",
    "    \n",
    "    bias_values_ua[i] = numpy.sqrt( numpy.mean((hist_field_ua - obs_field_ua)**2.) )\n",
    "    convergence_values_ua[i] = numpy.sqrt( numpy.mean((eoc_field_ua - model_field_mmem_ua)**2.) )\n",
    "    \n",
    "    bias_values_tel[i] = numpy.sqrt( numpy.mean((hist_field_tel - obs_field_tel)**2.) )\n",
    "    correlation_vals_tel[i] = scipy.stats.pearsonr(hist_field_tel.flatten(), obs_field_tel.flatten())[0]\n",
    "\n",
    "mmem_bias_pr = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_pr, axis=0) - obs_field_pr)**2. ))\n",
    "mmem_bias_ts = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_ts, axis=0) - obs_field_ts)**2. ))\n",
    "mmem_bias_ua = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_ua, axis=0) - obs_field_ua)**2. ))\n",
    "mmem_bias_tel = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_tel, axis=0) - obs_field_tel)**2. ))\n",
    "\n",
    "bias_values_pr_LENS = numpy.zeros((40))\n",
    "bias_values_ts_LENS = numpy.zeros((40))\n",
    "bias_values_ua_LENS = numpy.zeros((40))\n",
    "\n",
    "for i in range(40):\n",
    "    hist_field_pr = model_data_hist_pr_LENS[i,:,:]\n",
    "    hist_field_ts = model_data_hist_ts_LENS[i,:,:]\n",
    "    hist_field_ua = model_data_hist_ua_LENS[i,:,:]\n",
    "    \n",
    "    bias_values_pr_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_pr - obs_field_pr)**2.) )\n",
    "    bias_values_ts_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_ts - obs_field_ts)**2.) )\n",
    "    bias_values_ua_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_ua - obs_field_ua)**2.) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries to be used below\n",
    "dict_pr = {\n",
    "'bias_values_mods':bias_values_pr,\n",
    "'convergence_values_mods':convergence_values_pr,\n",
    "'bias_values_LENS':bias_values_pr_LENS,\n",
    "'mmem_bias':mmem_bias_pr,\n",
    "'nlat':pr_regional_nlat,\n",
    "'nlon':pr_regional_nlon,\n",
    "'fields_hist_mods':model_data_hist_pr,\n",
    "'fields_hist_mods_LENS':model_data_hist_pr_LENS,\n",
    "'fields_eoc_mods':model_data_eoc_pr,\n",
    "'obs_field':obs_field_pr,\n",
    "'LENS':True\n",
    "}\n",
    "\n",
    "dict_ts = {\n",
    "'bias_values_mods':bias_values_ts,\n",
    "'convergence_values_mods':convergence_values_ts,\n",
    "'bias_values_LENS':bias_values_ts_LENS,\n",
    "'mmem_bias':mmem_bias_ts,\n",
    "'nlat':ts_regional_nlat,\n",
    "'nlon':ts_regional_nlon,\n",
    "'fields_hist_mods':model_data_hist_ts,\n",
    "'fields_hist_mods_LENS':model_data_hist_ts_LENS,\n",
    "'fields_eoc_mods':model_data_eoc_ts,\n",
    "'obs_field':obs_field_ts,\n",
    "'LENS':True\n",
    "}\n",
    "\n",
    "dict_ua = {\n",
    "'bias_values_mods':bias_values_ua,\n",
    "'convergence_values_mods':convergence_values_ua,\n",
    "'bias_values_LENS':bias_values_ua_LENS,\n",
    "'mmem_bias':mmem_bias_ua,\n",
    "'nlat':ua_regional_nlat,\n",
    "'nlon':ua_regional_nlon,\n",
    "'fields_hist_mods':model_data_hist_ua,\n",
    "'fields_hist_mods_LENS':model_data_hist_ua_LENS,\n",
    "'fields_eoc_mods':model_data_eoc_ua,\n",
    "'obs_field':obs_field_ua,\n",
    "'LENS':True\n",
    "}\n",
    "\n",
    "dict_tel = {\n",
    "'bias_values_mods':bias_values_tel,\n",
    "'mmem_bias':mmem_bias_tel,\n",
    "'LENS':False,\n",
    "'nlat':tel_regional_nlat,\n",
    "'nlon':tel_regional_nlon,\n",
    "'fields_hist_mods':model_data_hist_tel,\n",
    "'obs_field':obs_field_tel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.axis3d import Axis\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.projections as proj\n",
    "from matplotlib.colors import colorConverter\n",
    "\n",
    "class axis3d_custom(Axis):\n",
    "    def __init__(self, adir, v_intervalx, d_intervalx, axes, *args, **kwargs):\n",
    "        Axis.__init__(self, adir, v_intervalx, d_intervalx, axes, *args, **kwargs)\n",
    "        self.gridline_colors = []\n",
    "    def set_gridline_color(self, *gridline_info):\n",
    "        '''Gridline_info is a tuple containing the value of the gridline to change\n",
    "        and the color to change it to. A list of tuples may be used with the * operator.'''\n",
    "        self.gridline_colors.extend(gridline_info)\n",
    "    def draw(self, renderer):\n",
    "        # filter locations here so that no extra grid lines are drawn\n",
    "        Axis.draw(self, renderer)\n",
    "        which_gridlines = []\n",
    "        if self.gridline_colors:\n",
    "            locmin, locmax = self.get_view_interval()\n",
    "            if locmin > locmax:\n",
    "                locmin, locmax = locmax, locmin\n",
    "\n",
    "            # Rudimentary clipping\n",
    "            majorLocs = [loc for loc in self.major.locator() if\n",
    "                         locmin <= loc <= locmax]\n",
    "            for i, val in enumerate(majorLocs):\n",
    "                for colored_val, color in self.gridline_colors:\n",
    "                    if val == colored_val:\n",
    "                        which_gridlines.append((i, color))\n",
    "            colors = self.gridlines.get_colors()\n",
    "            for val, color in which_gridlines:\n",
    "                colors[val] = colorConverter.to_rgba(color)\n",
    "            self.gridlines.set_color(colors)\n",
    "            self.gridlines.draw(renderer, project=True)\n",
    "\n",
    "class XAxis(axis3d_custom):\n",
    "    def get_data_interval(self):\n",
    "        'return the Interval instance for this axis data limits'\n",
    "        return self.axes.xy_dataLim.intervalx\n",
    "\n",
    "class YAxis(axis3d_custom):\n",
    "    def get_data_interval(self):\n",
    "        'return the Interval instance for this axis data limits'\n",
    "        return self.axes.xy_dataLim.intervaly\n",
    "\n",
    "class ZAxis(axis3d_custom):\n",
    "    def get_data_interval(self):\n",
    "        'return the Interval instance for this axis data limits'\n",
    "        return self.axes.zz_dataLim.intervalx\n",
    "\n",
    "class Axes3D_custom(Axes3D):\n",
    "    \"\"\"\n",
    "    3D axes object.\n",
    "    \"\"\"\n",
    "    name = '3d_custom'\n",
    "\n",
    "    def _init_axis(self):\n",
    "        '''Init 3D axes; overrides creation of regular X/Y axes'''\n",
    "        self.w_xaxis = XAxis('x', self.xy_viewLim.intervalx,\n",
    "                            self.xy_dataLim.intervalx, self)\n",
    "        self.xaxis = self.w_xaxis\n",
    "        self.w_yaxis = YAxis('y', self.xy_viewLim.intervaly,\n",
    "                            self.xy_dataLim.intervaly, self)\n",
    "        self.yaxis = self.w_yaxis\n",
    "        self.w_zaxis = ZAxis('z', self.zz_viewLim.intervalx,\n",
    "                            self.zz_dataLim.intervalx, self)\n",
    "        self.zaxis = self.w_zaxis\n",
    "\n",
    "        for ax in self.xaxis, self.yaxis, self.zaxis:\n",
    "            ax.init3d()\n",
    "proj.projection_registry.register(Axes3D_custom)\n",
    "\n",
    "\n",
    "# ###patch start###\n",
    "# from mpl_toolkits.mplot3d.axis3d import Axis\n",
    "# if not hasattr(Axis, \"_get_coord_info_old\"):\n",
    "#     def _get_coord_info_new(self, renderer):\n",
    "#         mins, maxs, centers, deltas, tc, highs = self._get_coord_info_old(renderer)\n",
    "#         mins += deltas / 4\n",
    "#         maxs -= deltas / 4\n",
    "#         return mins, maxs, centers, deltas, tc, highs\n",
    "#     Axis._get_coord_info_old = Axis._get_coord_info  \n",
    "#     Axis._get_coord_info = _get_coord_info_new\n",
    "# ###patch end###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pareto_set_collect_2d_list = []\n",
    "pareto_set_collect_3d_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RASTERIZED VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate all biases for CMIP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATESTRING = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "dict_x=dict_pr\n",
    "dict_y=dict_ts\n",
    "dict_z=dict_ua\n",
    "\n",
    "N_pareto_loops=5\n",
    "\n",
    "# do N choose k subensembles\n",
    "# for each, calculate ensemble mean\n",
    "\n",
    "k=5\n",
    "\n",
    "model_numbers = numpy.arange(nmods, dtype=numpy.int)\n",
    "model_combinations = list(itertools.combinations(model_numbers, k))\n",
    "random.shuffle(model_combinations)\n",
    "model_combinations = numpy.array(model_combinations, dtype=numpy.int)\n",
    "\n",
    "N_ens = model_combinations.shape[0]\n",
    "model_combinations = model_combinations[0:N_ens,:]\n",
    "\n",
    "subensembles_hist_x = numpy.zeros((N_ens, dict_x['nlat'], dict_x['nlon']))\n",
    "subensembles_hist_y = numpy.zeros((N_ens, dict_y['nlat'], dict_y['nlon']))\n",
    "subensembles_hist_z = numpy.zeros((N_ens, dict_z['nlat'], dict_z['nlon']))\n",
    "\n",
    "for i in range(N_ens):\n",
    "    subensembles_hist_x[i,:,:] = numpy.mean(dict_x['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "    subensembles_hist_y[i,:,:] = numpy.mean(dict_y['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "    subensembles_hist_z[i,:,:] = numpy.mean(dict_z['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "\n",
    "bias_values_subensembles_x = numpy.zeros((N_ens))\n",
    "bias_values_subensembles_y = numpy.zeros((N_ens))\n",
    "bias_values_subensembles_z = numpy.zeros((N_ens))\n",
    "\n",
    "for i in range(N_ens):\n",
    "    hist_field_x = subensembles_hist_x[i,:,:]\n",
    "    hist_field_y = subensembles_hist_y[i,:,:]\n",
    "    hist_field_z = subensembles_hist_z[i,:,:]\n",
    "\n",
    "    bias_values_subensembles_x[i] = numpy.sqrt( numpy.mean( (hist_field_x - dict_x['obs_field'])**2.) )\n",
    "    bias_values_subensembles_y[i] = numpy.sqrt( numpy.mean( (hist_field_y - dict_y['obs_field'])**2.) )\n",
    "    bias_values_subensembles_z[i] = numpy.sqrt( numpy.mean( (hist_field_z - dict_z['obs_field'])**2.) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Pareto information (which_combo in 2D then 3D last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating Pareto front for 2D combo 1\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n",
      "calculating Pareto front for 2D combo 2\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n",
      "calculating Pareto front for 2D combo 3\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n",
      "calculating 3D Pareto front\n",
      "calculating first Pareto front for 3D surface\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n"
     ]
    }
   ],
   "source": [
    "for which_combo in [1,2,3]:\n",
    "    \n",
    "    ##########\n",
    "    # CALCULATING PARETO FRONT INFO\n",
    "    # FIRST PARETO LOOP IS DONE HERE \n",
    "    print('calculating Pareto front for 2D combo '+str(which_combo))\n",
    "\n",
    "    if which_combo==1:\n",
    "        pareto_array = numpy.vstack((bias_values_subensembles_x, bias_values_subensembles_y)).T\n",
    "    elif which_combo==2:\n",
    "        pareto_array = numpy.vstack((bias_values_subensembles_x, bias_values_subensembles_z)).T\n",
    "    elif which_combo==3:\n",
    "        pareto_array = numpy.vstack((bias_values_subensembles_y, bias_values_subensembles_z)).T\n",
    "    numpy.savetxt('crap.txt', pareto_array, delimiter=',')\n",
    "    os.system(\"python pareto.py crap.txt --delimiter=',' --output='crap_pareto_set.txt'\")\n",
    "    pareto_set = numpy.loadtxt('crap_pareto_set.txt', delimiter=',')\n",
    "    n_optima = pareto_set.shape[0]\n",
    "    n_col = pareto_set.shape[1]\n",
    "\n",
    "    pareto_set_collect = numpy.empty((0,2))\n",
    "    pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "\n",
    "    if which_combo==1:\n",
    "        col1_orig = numpy.copy(bias_values_subensembles_x)\n",
    "        col2_orig = numpy.copy(bias_values_subensembles_y)\n",
    "        col1 = numpy.copy(bias_values_subensembles_x)\n",
    "        col2 = numpy.copy(bias_values_subensembles_y)\n",
    "    elif which_combo==2:\n",
    "        col1_orig = numpy.copy(bias_values_subensembles_x)\n",
    "        col2_orig = numpy.copy(bias_values_subensembles_z)\n",
    "        col1 = numpy.copy(bias_values_subensembles_x)\n",
    "        col2 = numpy.copy(bias_values_subensembles_z)\n",
    "    elif which_combo==3:\n",
    "        col1_orig = numpy.copy(bias_values_subensembles_y)\n",
    "        col2_orig = numpy.copy(bias_values_subensembles_z)\n",
    "        col1 = numpy.copy(bias_values_subensembles_y)\n",
    "        col2 = numpy.copy(bias_values_subensembles_z)\n",
    "\n",
    "    # EXTRA PARETO FRONTS ARE DONE HERE, AS LONG AS N_pareto_loops>=1\n",
    "    for loop in range(1,N_pareto_loops):\n",
    "        print('calculating Pareto front '+str(loop+1))\n",
    "        # now find indices where this front occurs\n",
    "        set_indices = numpy.zeros((pareto_set.shape[0]), dtype=numpy.int)\n",
    "        for i in range(pareto_set.shape[0]):\n",
    "            set_indices[i] = numpy.where( (col1==pareto_set[i,0])&(col2==pareto_set[i,1]) )[0]\n",
    "        # and then get rid of them\n",
    "        col1[set_indices] = 999.\n",
    "        col2[set_indices] = 999.\n",
    "\n",
    "        pareto_array = numpy.vstack((col1, col2)).T\n",
    "        numpy.savetxt('crap.txt', pareto_array, delimiter=',')\n",
    "        os.system(\"python pareto.py crap.txt --delimiter=',' --output='crap_pareto_set.txt'\")\n",
    "        pareto_set = numpy.loadtxt('crap_pareto_set.txt', delimiter=',')\n",
    "\n",
    "        pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "\n",
    "        n_col = pareto_set.shape[1]    \n",
    "        n_optima = pareto_set_collect.shape[0]\n",
    "    \n",
    "    pareto_set_collect_2d_list.append(pareto_set_collect)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PARETO CALCULATIONS IN 3D\n",
    "print('calculating 3D Pareto front')\n",
    "\n",
    "dict_x=dict_pr\n",
    "dict_y=dict_ts\n",
    "dict_z=dict_ua\n",
    "\n",
    "##########\n",
    "# CALCULATING PARETO FRONT INFO\n",
    "# FIRST PARETO LOOP IS DONE HERE\n",
    "print('calculating first Pareto front for 3D surface')\n",
    "pareto_array = numpy.vstack((bias_values_subensembles_x, bias_values_subensembles_y, bias_values_subensembles_z)).T\n",
    "numpy.savetxt('crap.txt', pareto_array, delimiter=',')\n",
    "os.system(\"python pareto.py crap.txt --delimiter=',' --output='crap_pareto_set.txt'\")\n",
    "pareto_set = numpy.loadtxt('crap_pareto_set.txt', delimiter=',')\n",
    "\n",
    "# collect indices\n",
    "set_indices = numpy.zeros(pareto_set.shape[0], dtype=numpy.int)\n",
    "for i in range(pareto_set.shape[0]):\n",
    "    #a = numpy.where( (bias_values_subensembles_x==pareto_set[i,0])&(bias_values_subensembles_y==pareto_set[i,1])&(bias_values_subensembles_z==pareto_set[i,2]) )[0][0]\n",
    "    #print(a)\n",
    "    set_indices[i] = numpy.where( (bias_values_subensembles_x==pareto_set[i,0])&(bias_values_subensembles_y==pareto_set[i,1])&(bias_values_subensembles_z==pareto_set[i,2]) )[0][0]\n",
    "\n",
    "pareto_set_sizes_3d=[]\n",
    "n_optima = pareto_set.shape[0]\n",
    "pareto_set_sizes_3d.append(n_optima)\n",
    "n_col = pareto_set.shape[1]\n",
    "\n",
    "pareto_set_collect = numpy.empty((0,3))\n",
    "pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "set_indices_collect = numpy.empty((0))\n",
    "set_indices_collect = numpy.append(set_indices_collect, set_indices)\n",
    "\n",
    "col1_orig = numpy.copy(bias_values_subensembles_x)\n",
    "col2_orig = numpy.copy(bias_values_subensembles_y)\n",
    "col3_orig = numpy.copy(bias_values_subensembles_z)\n",
    "col1 = numpy.copy(bias_values_subensembles_x)\n",
    "col2 = numpy.copy(bias_values_subensembles_y)\n",
    "col3 = numpy.copy(bias_values_subensembles_z)\n",
    "\n",
    "col1[set_indices] = 999.\n",
    "col2[set_indices] = 999.\n",
    "col3[set_indices] = 999.\n",
    "\n",
    "# EXTRA PARETO FRONTS ARE DONE HERE, AS LONG AS N_pareto_loops>=1\n",
    "for loop in range(1,N_pareto_loops):\n",
    "    print('calculating Pareto front '+str(loop+1))\n",
    "    # now find indices where this front occurs\n",
    "\n",
    "    pareto_array = numpy.vstack((col1, col2, col3)).T\n",
    "    numpy.savetxt('crap.txt', pareto_array, delimiter=',')\n",
    "    os.system(\"python pareto.py crap.txt --delimiter=',' --output='crap_pareto_set.txt'\")\n",
    "    pareto_set = numpy.loadtxt('crap_pareto_set.txt', delimiter=',')\n",
    "\n",
    "    pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "\n",
    "    set_indices = numpy.zeros(pareto_set.shape[0], dtype=numpy.int)\n",
    "    for i in range(pareto_set.shape[0]):\n",
    "        set_indices[i] = numpy.where( (col1==pareto_set[i,0])&(col2==pareto_set[i,1])&(col3==pareto_set[i,2]) )[0][0]\n",
    "    set_indices_collect = numpy.append(set_indices_collect, set_indices)\n",
    "\n",
    "    n_col = pareto_set.shape[1]    \n",
    "    n_optima = pareto_set_collect.shape[0]\n",
    "    pareto_set_sizes_3d.append(pareto_set.shape[0])\n",
    "\n",
    "    col1[set_indices] = 999.\n",
    "    col2[set_indices] = 999.\n",
    "    col3[set_indices] = 999.\n",
    "    \n",
    "pareto_set_collect_3d_list.append(pareto_set_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate all biases for LENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATESTRING = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "dict_x=dict_pr\n",
    "dict_y=dict_ts\n",
    "dict_z=dict_ua\n",
    "\n",
    "#N_pareto_loops=5\n",
    "\n",
    "# do N choose k subensembles\n",
    "# for each, calculate ensemble mean\n",
    "\n",
    "#k=5\n",
    "\n",
    "model_numbers = numpy.arange(40, dtype=numpy.int)\n",
    "model_combinations_LENS = list(itertools.combinations(model_numbers, k))\n",
    "random.shuffle(model_combinations_LENS)\n",
    "model_combinations_LENS = numpy.array(model_combinations_LENS, dtype=numpy.int)\n",
    "\n",
    "N_ens_LENS = model_combinations_LENS.shape[0]\n",
    "model_combinations_LENS = model_combinations_LENS[0:N_ens_LENS,:]\n",
    "\n",
    "subensembles_hist_x_LENS = numpy.zeros((N_ens_LENS, dict_x['nlat'], dict_x['nlon']))\n",
    "subensembles_hist_y_LENS = numpy.zeros((N_ens_LENS, dict_y['nlat'], dict_y['nlon']))\n",
    "subensembles_hist_z_LENS = numpy.zeros((N_ens_LENS, dict_z['nlat'], dict_z['nlon']))\n",
    "\n",
    "for i in range(N_ens_LENS):\n",
    "    subensembles_hist_x_LENS[i,:,:] = numpy.mean(dict_x['fields_hist_mods_LENS'][model_combinations_LENS[i,:],:,:], axis=0)\n",
    "    subensembles_hist_y_LENS[i,:,:] = numpy.mean(dict_y['fields_hist_mods_LENS'][model_combinations_LENS[i,:],:,:], axis=0)\n",
    "    subensembles_hist_z_LENS[i,:,:] = numpy.mean(dict_z['fields_hist_mods_LENS'][model_combinations_LENS[i,:],:,:], axis=0)\n",
    "\n",
    "bias_values_subensembles_x_LENS = numpy.zeros((N_ens_LENS))\n",
    "bias_values_subensembles_y_LENS = numpy.zeros((N_ens_LENS))\n",
    "bias_values_subensembles_z_LENS = numpy.zeros((N_ens_LENS))\n",
    "\n",
    "for i in range(N_ens_LENS):\n",
    "    hist_field_x = subensembles_hist_x_LENS[i,:,:]\n",
    "    hist_field_y = subensembles_hist_y_LENS[i,:,:]\n",
    "    hist_field_z = subensembles_hist_z_LENS[i,:,:]\n",
    "\n",
    "    bias_values_subensembles_x_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_x - dict_x['obs_field'])**2.) )\n",
    "    bias_values_subensembles_y_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_y - dict_y['obs_field'])**2.) )\n",
    "    bias_values_subensembles_z_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_z - dict_z['obs_field'])**2.) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658008\n"
     ]
    }
   ],
   "source": [
    "save_dict = {}\n",
    "\n",
    "save_dict['pareto_set_collect_2d_list'] = pareto_set_collect_2d_list\n",
    "save_dict['pareto_set_collect_3d_list'] = pareto_set_collect_3d_list\n",
    "\n",
    "save_dict['bias_values_subensembles_x'] = bias_values_subensembles_x\n",
    "save_dict['bias_values_subensembles_y'] = bias_values_subensembles_y\n",
    "save_dict['bias_values_subensembles_z'] = bias_values_subensembles_z\n",
    "\n",
    "save_dict['bias_values_subensembles_x_LENS'] = bias_values_subensembles_x_LENS\n",
    "save_dict['bias_values_subensembles_y_LENS'] = bias_values_subensembles_y_LENS\n",
    "save_dict['bias_values_subensembles_z_LENS'] = bias_values_subensembles_z_LENS\n",
    "\n",
    "save_dict['k'] = k\n",
    "save_dict['N_pareto_loops'] = N_pareto_loops\n",
    "\n",
    "save_dict['N_ens'] = N_ens\n",
    "save_dict['model_combinations'] = model_combinations\n",
    "\n",
    "save_dict['N_ens_LENS'] = N_ens_LENS\n",
    "save_dict['model_combinations_LENS'] = model_combinations_LENS\n",
    "\n",
    "save_dict['dict_x'] = dict_x\n",
    "save_dict['dict_y'] = dict_y\n",
    "save_dict['dict_z'] = dict_z\n",
    "\n",
    "save_dict['pareto_set_sizes_3d'] = pareto_set_sizes_3d\n",
    "\n",
    "save_dir = '/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/figures_and_analysis/figure3/data_files/'\n",
    "save_filename = 'pareto_front_results_'+DATESTRING+'.npy'\n",
    "numpy.save(save_dir + save_filename, save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-10_20:52:51\n"
     ]
    }
   ],
   "source": [
    "print(DATESTRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Pareto-optimality information"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "set_indices_collect = numpy.array(set_indices_collect, dtype=numpy.int)\n",
    "\n",
    "pareto_optimal_indices = model_combinations[set_indices_collect,:]\n",
    "\n",
    "nr,nc = pareto_optimal_indices.shape\n",
    "working_dir = '/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/figures_and_analysis/data_files/'\n",
    "filename = 'pareto_optimal_indices_'+DATESTRING+'_size_'+str(nr)+'x'+str(nc)+'.npy'\n",
    "\n",
    "numpy.save(working_dir+filename, pareto_optimal_indices)\n",
    "\n",
    "filename = 'all_model_combinations_'+DATESTRING+'.npy'\n",
    "numpy.save(working_dir+filename, model_combinations)\n",
    "\n",
    "filename = 'pareto_set_collect_3D_'+DATESTRING+'.npy'\n",
    "numpy.save(working_dir+filename, pareto_set_collect)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(bias_sort)\n",
    "print(model_names[bias_sort])\n",
    "\n",
    "numpy.save('/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/figures_and_analysis/data_files/bias_sort_indices.npy', bias_sort)\n",
    "numpy.save('/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/figures_and_analysis/data_files/model_names_sorted.npy', model_names[bias_sort])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening datestring specific information"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "DATESTRING = '2017-07-05_18:52:44'\n",
    "working_dir = '/Users/baird/Dropbox/_analysis/subensembles/NEW_CALCULATIONS/figures_and_analysis/data_files/'\n",
    "nr,nc = 1506, 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bias_sort = numpy.load('data_files/bias_sort_indices.npy')\n",
    "model_names_sorted = numpy.load('data_files/model_names_sorted.npy')\n",
    "\n",
    "pareto_optimal_indices = numpy.load(working_dir + 'pareto_optimal_indices_'+DATESTRING+'_size_'+str(nr)+'x'+str(nc)+'_index_LARGE_domains.npy')\n",
    "model_combinations = numpy.load(working_dir + 'all_model_combinations_'+DATESTRING+'_index_LARGE_domains.npy')\n",
    "pareto_set_collect = numpy.load(working_dir + 'pareto_set_collect_3D_'+DATESTRING+'_index_LARGE_domains.npy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(pareto_optimal_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now take the pareto-optimal sets and use them to plot on other things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set which_combo here for maps of percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing for 3D Pareto front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get all the subensembles in pareto_set_collect, take their mean, and look at end-of-century precip change\n",
    "# GET INFO ON OPTIMAL PARETO SETS, ETC.\n",
    "\n",
    "# SAVE THE MODELS THAT MAKE UP THE PARETO FRONT\n",
    "n_optima = pareto_set_collect.shape[0]\n",
    "optimal_subensembles = numpy.empty((n_optima, k), dtype=object)\n",
    "optimal_subensembles_indices = numpy.empty((n_optima, k), dtype=int)\n",
    "\n",
    "optimal_y_subensembles = numpy.empty((N_pareto_loops, k), dtype=object)\n",
    "optimal_x_subensembles = numpy.empty((N_pareto_loops, k), dtype=object)\n",
    "optimal_z_subensembles = numpy.empty((N_pareto_loops, k), dtype=object)\n",
    "\n",
    "optimal_y_pareto_combos = numpy.zeros(N_pareto_loops, dtype=numpy.int)\n",
    "optimal_x_pareto_combos = numpy.zeros(N_pareto_loops, dtype=numpy.int)\n",
    "optimal_z_pareto_combos = numpy.zeros(N_pareto_loops, dtype=numpy.int)\n",
    "\n",
    "for i in range(n_optima):\n",
    "    pareto_combo = numpy.where((col1_orig==pareto_set_collect[i,0]) & (col2_orig==pareto_set_collect[i,1]) & (col3_orig==pareto_set_collect[i,2]))[0][0]\n",
    "    optimal_subensembles[i,:] = model_names[model_combinations[pareto_combo,:]]\n",
    "    optimal_subensembles_indices[i,:] = model_combinations[pareto_combo,:]\n",
    "\n",
    "optimal_subensembles = optimal_subensembles.astype(numpy.str)\n",
    "\n",
    "print(optimal_subensembles.shape)\n",
    "# calculate precip change over each optimal subensemble\n",
    "all_po_subensembles_hist = numpy.zeros((optimal_subensembles.shape[0], pr_regional_nlat, pr_regional_nlon))\n",
    "all_po_subensembles_eoc = numpy.zeros((optimal_subensembles.shape[0], pr_regional_nlat, pr_regional_nlon))\n",
    "individual_subensembles_hist = numpy.zeros((k, pr_regional_nlat, pr_regional_nlon))\n",
    "individual_subensembles_eoc = numpy.zeros((k, pr_regional_nlat, pr_regional_nlon))\n",
    "for ens in range(optimal_subensembles.shape[0]):\n",
    "    for member in range(k):\n",
    "        individual_subensembles_hist[member,:,:] = model_data_hist_pr[numpy.where(optimal_subensembles[ens,member]==model_names)[0][0],:,:]\n",
    "        individual_subensembles_eoc[member,:,:] = model_data_eoc_pr[numpy.where(optimal_subensembles[ens,member]==model_names)[0][0],:,:]\n",
    "    all_po_subensembles_hist[ens,:,:] = numpy.mean(individual_subensembles_hist, axis=0)\n",
    "    all_po_subensembles_eoc[ens,:,:] = numpy.mean(individual_subensembles_eoc, axis=0)\n",
    "#print(pareto_set_collect)\n",
    "#mp.contourf(all_subensembles[0,:,:])\n",
    "#mp.contourf(numpy.mean(all_subensembles_hist, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now stored as optimal_subensembles_indices and optimal_subensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(optimal_subensembles_indices)\n",
    "# find ccal indices on this graph\n",
    "print(ncal_latlon)\n",
    "print(pr_regional_lat_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through the optimal subensembles\n",
    "# pull out ccal, ncal, scal lat/lon indices, take average\n",
    "# look at them\n",
    "\n",
    "# get latlon combo\n",
    "latlon_indices_ncal = numpy.zeros((ncal_latlon.shape),dtype=int)\n",
    "latlon_indices_ccal = numpy.zeros((ccal_latlon.shape),dtype=int)\n",
    "latlon_indices_scal = numpy.zeros((scal_latlon.shape),dtype=int)\n",
    "latlon_indices_coastal_cal = numpy.zeros((coastal_cal_latlon.shape), dtype=int)\n",
    "\n",
    "for i in range(ncal_latlon.shape[0]):\n",
    "    latlon_indices_ncal[i,0] = numpy.where(ncal_latlon[i,0]==pr_regional_lon_vals)[0][0]\n",
    "    latlon_indices_ncal[i,1] = numpy.where(ncal_latlon[i,1]==pr_regional_lat_vals)[0][0]\n",
    "\n",
    "    latlon_indices_ccal[i,0] = numpy.where(ccal_latlon[i,0]==pr_regional_lon_vals)[0][0]\n",
    "    latlon_indices_ccal[i,1] = numpy.where(ccal_latlon[i,1]==pr_regional_lat_vals)[0][0]\n",
    "\n",
    "    latlon_indices_scal[i,0] = numpy.where(scal_latlon[i,0]==pr_regional_lon_vals)[0][0]\n",
    "    latlon_indices_scal[i,1] = numpy.where(scal_latlon[i,1]==pr_regional_lat_vals)[0][0]\n",
    "\n",
    "for i in range(coastal_cal_latlon.shape[0]):\n",
    "    latlon_indices_coastal_cal[i,0] = numpy.where(coastal_cal_latlon[i,0]==pr_regional_lon_vals)[0][0]\n",
    "    latlon_indices_coastal_cal[i,1] = numpy.where(coastal_cal_latlon[i,1]==pr_regional_lat_vals)[0][0]\n",
    "\n",
    "all_po_subensembles_hist[0,latlon_indices_ncal[:,0],latlon_indices_ncal[:,1]]\n",
    "print(latlon_indices_coastal_cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking Pareto-optimal subensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_po_subensembles_hist_ncal = all_po_subensembles_hist[:,latlon_indices_ncal[:,1],latlon_indices_ncal[:,0]].mean(axis=1)\n",
    "all_po_subensembles_hist_ccal = all_po_subensembles_hist[:,latlon_indices_ccal[:,1],latlon_indices_ccal[:,0]].mean(axis=1)\n",
    "all_po_subensembles_hist_scal = all_po_subensembles_hist[:,latlon_indices_scal[:,1],latlon_indices_scal[:,0]].mean(axis=1)\n",
    "\n",
    "all_po_subensembles_eoc_ncal = all_po_subensembles_eoc[:,latlon_indices_ncal[:,1],latlon_indices_ncal[:,0]].mean(axis=1)\n",
    "all_po_subensembles_eoc_ccal = all_po_subensembles_eoc[:,latlon_indices_ccal[:,1],latlon_indices_ccal[:,0]].mean(axis=1)\n",
    "all_po_subensembles_eoc_scal = all_po_subensembles_eoc[:,latlon_indices_scal[:,1],latlon_indices_scal[:,0]].mean(axis=1)\n",
    "\n",
    "all_po_subensembles_hist_ncal_mean = numpy.mean(all_po_subensembles_hist_ncal, axis=0)\n",
    "all_po_subensembles_hist_ccal_mean = numpy.mean(all_po_subensembles_hist_ccal, axis=0)\n",
    "all_po_subensembles_hist_scal_mean = numpy.mean(all_po_subensembles_hist_scal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_po_subensembles_hist_coastal_cal = all_po_subensembles_hist[:,latlon_indices_coastal_cal[:,1],latlon_indices_coastal_cal[:,0]]\n",
    "all_po_subensembles_eoc_coastal_cal = all_po_subensembles_eoc[:,latlon_indices_coastal_cal[:,1],latlon_indices_coastal_cal[:,0]]\n",
    "all_po_subensembles_hist_coastal_cal_mean = numpy.mean(all_po_subensembles_hist_coastal_cal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_subensembles_hist_pr = numpy.zeros((N_ens, dict_pr['nlat'], dict_pr['nlon']))\n",
    "ALL_subensembles_eoc_pr = numpy.zeros((N_ens, dict_pr['nlat'], dict_pr['nlon']))\n",
    "for i in range(N_ens):\n",
    "    ALL_subensembles_hist_pr[i,:,:] = numpy.mean(dict_pr['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "    ALL_subensembles_eoc_pr[i,:,:] = numpy.mean(dict_pr['fields_eoc_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "\n",
    "ALL_subensembles_hist_pr_ncal = ALL_subensembles_hist_pr[:,latlon_indices_ncal[:,1],latlon_indices_ncal[:,0]].mean(axis=1)\n",
    "ALL_subensembles_hist_pr_ccal = ALL_subensembles_hist_pr[:,latlon_indices_ccal[:,1],latlon_indices_ccal[:,0]].mean(axis=1)\n",
    "ALL_subensembles_hist_pr_scal = ALL_subensembles_hist_pr[:,latlon_indices_scal[:,1],latlon_indices_scal[:,0]].mean(axis=1)\n",
    "\n",
    "ALL_subensembles_eoc_pr_ncal = ALL_subensembles_eoc_pr[:,latlon_indices_ncal[:,1],latlon_indices_ncal[:,0]].mean(axis=1)\n",
    "ALL_subensembles_eoc_pr_ccal = ALL_subensembles_eoc_pr[:,latlon_indices_ccal[:,1],latlon_indices_ccal[:,0]].mean(axis=1)\n",
    "ALL_subensembles_eoc_pr_scal = ALL_subensembles_eoc_pr[:,latlon_indices_scal[:,1],latlon_indices_scal[:,0]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take coastal California coastal grid boxes (5 total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_subensembles_hist_pr_coastal_cal = ALL_subensembles_hist_pr[:,latlon_indices_coastal_cal[:,1],latlon_indices_coastal_cal[:,0]]\n",
    "ALL_subensembles_eoc_pr_coastal_cal = ALL_subensembles_eoc_pr[:,latlon_indices_coastal_cal[:,1],latlon_indices_coastal_cal[:,0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(ALL_subensembles_hist_pr_coastal_cal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cmip5 full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmip5_hist_pr_ncal = dict_pr['fields_hist_mods'][:,latlon_indices_ncal[:,1],latlon_indices_ncal[:,0]].mean(axis=1)\n",
    "cmip5_hist_pr_ccal = dict_pr['fields_hist_mods'][:,latlon_indices_ccal[:,1],latlon_indices_ccal[:,0]].mean(axis=1)\n",
    "cmip5_hist_pr_scal = dict_pr['fields_hist_mods'][:,latlon_indices_scal[:,1],latlon_indices_scal[:,0]].mean(axis=1)\n",
    "\n",
    "cmip5_eoc_pr_ncal = dict_pr['fields_eoc_mods'][:,latlon_indices_ncal[:,1],latlon_indices_ncal[:,0]].mean(axis=1)\n",
    "cmip5_eoc_pr_ccal = dict_pr['fields_eoc_mods'][:,latlon_indices_ccal[:,1],latlon_indices_ccal[:,0]].mean(axis=1)\n",
    "cmip5_eoc_pr_scal = dict_pr['fields_eoc_mods'][:,latlon_indices_scal[:,1],latlon_indices_scal[:,0]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmip5_hist_pr_coastal_cal = dict_pr['fields_hist_mods'][:,latlon_indices_coastal_cal[:,1],latlon_indices_coastal_cal[:,0]]\n",
    "cmip5_eoc_pr_coastal_cal = dict_pr['fields_eoc_mods'][:,latlon_indices_coastal_cal[:,1],latlon_indices_coastal_cal[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cmip5_hist_pr_coastal_cal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the California index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subensembles_delta_pr_ncal = numpy.zeros((N_ens)) # new\n",
    "subensembles_delta_pr_ccal = numpy.zeros((N_ens)) # new\n",
    "subensembles_delta_pr_scal = numpy.zeros((N_ens)) # new\n",
    "subensembles_delta_pr_coastal_cal = numpy.zeros((N_ens,5)) # new\n",
    "\n",
    "for i in range(N_ens):\n",
    "    # new\n",
    "    hist = ALL_subensembles_hist_pr_ncal[i]\n",
    "    eoc = ALL_subensembles_eoc_pr_ncal[i]\n",
    "    subensembles_delta_pr_ncal[i] = eoc-hist\n",
    "    hist = ALL_subensembles_hist_pr_ccal[i]\n",
    "    eoc = ALL_subensembles_eoc_pr_ccal[i]\n",
    "    subensembles_delta_pr_ccal[i] = eoc-hist\n",
    "    hist = ALL_subensembles_hist_pr_scal[i]\n",
    "    eoc = ALL_subensembles_eoc_pr_scal[i]\n",
    "    subensembles_delta_pr_scal[i] = eoc-hist\n",
    "    hist = ALL_subensembles_hist_pr_coastal_cal[i,:]\n",
    "    eoc = ALL_subensembles_eoc_pr_coastal_cal[i,:]\n",
    "    subensembles_delta_pr_coastal_cal[i,:] = eoc-hist\n",
    "\n",
    "# individual models\n",
    "model_delta_pr_ncal = numpy.zeros(nmods)\n",
    "model_delta_pr_ccal = numpy.zeros(nmods)\n",
    "model_delta_pr_scal = numpy.zeros(nmods)\n",
    "model_delta_pr_coastal_cal = numpy.zeros((nmods,5))\n",
    "\n",
    "for i in range(nmods):\n",
    "    hist = cmip5_hist_pr_ncal[i]\n",
    "    eoc = cmip5_eoc_pr_ncal[i]\n",
    "    model_delta_pr_ncal[i] = eoc-hist\n",
    "    \n",
    "    hist = cmip5_hist_pr_ccal[i]\n",
    "    eoc = cmip5_eoc_pr_ccal[i]\n",
    "    model_delta_pr_ccal[i] = eoc-hist\n",
    "\n",
    "    hist = cmip5_hist_pr_scal[i]\n",
    "    eoc = cmip5_eoc_pr_scal[i]\n",
    "    model_delta_pr_scal[i] = eoc-hist\n",
    "\n",
    "    hist = cmip5_hist_pr_coastal_cal[i,:]\n",
    "    eoc = cmip5_eoc_pr_coastal_cal[i,:]\n",
    "    model_delta_pr_coastal_cal[i,:] = eoc-hist\n",
    "\n",
    "\n",
    "#optimal subensembles\n",
    "\n",
    "optimal_subensembles_delta_pr_ncal = numpy.zeros(optimal_subensembles.shape[0])\n",
    "optimal_subensembles_delta_pr_ccal = numpy.zeros(optimal_subensembles.shape[0])\n",
    "optimal_subensembles_delta_pr_scal = numpy.zeros(optimal_subensembles.shape[0])\n",
    "optimal_subensembles_delta_pr_coastal_cal = numpy.zeros((optimal_subensembles.shape[0],5))\n",
    "\n",
    "for i in range(optimal_subensembles.shape[0]):\n",
    "    hist = all_po_subensembles_hist_ncal[i]\n",
    "    eoc = all_po_subensembles_eoc_ncal[i]\n",
    "    optimal_subensembles_delta_pr_ncal[i] = eoc-hist\n",
    "    \n",
    "    hist = all_po_subensembles_hist_ccal[i]\n",
    "    eoc = all_po_subensembles_eoc_ccal[i]\n",
    "    optimal_subensembles_delta_pr_ccal[i] = eoc-hist\n",
    "    \n",
    "    hist = all_po_subensembles_hist_scal[i]\n",
    "    eoc = all_po_subensembles_eoc_scal[i]\n",
    "    optimal_subensembles_delta_pr_scal[i] = eoc-hist\n",
    "\n",
    "    hist = all_po_subensembles_hist_coastal_cal[i,:]\n",
    "    eoc = all_po_subensembles_eoc_coastal_cal[i,:]\n",
    "    optimal_subensembles_delta_pr_coastal_cal[i,:] = eoc-hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOOOOOOOOSE PERCENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lo_perc = 10\n",
    "hi_perc = 90\n",
    "med_perc = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "fig = mp.figure(figsize=(8.5,3))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.axhline(0,lw=2,c='0.5', zorder=1)\n",
    "\n",
    "boxplot_width=[0.5]\n",
    "#ax.scatter([1]*N_ens, subensembles_delta_pr_ncal, zorder=2)\n",
    "#ax.scatter([2]*N_ens, subensembles_delta_pr_ccal, zorder=2)\n",
    "#ax.scatter([3]*N_ens, subensembles_delta_pr_scal, zorder=2)\n",
    "\n",
    "flierprops = dict(marker='.', markeredgecolor='0.1', markerfacecolor='0.1')\n",
    "medianprops = dict(linewidth=2, color='firebrick')\n",
    "boxprops = dict(linewidth=2, color='0.1')\n",
    "whiskerprops = dict(linewidth=2, color='0.1')\n",
    "capprops = dict(linewidth=2, color='0.1')\n",
    "\n",
    "whisk_range = [lo_perc, hi_perc]\n",
    "#whisk_range = 'range'\n",
    "\n",
    "ax.boxplot(model_delta_pr_ncal, zorder=2, positions=[1.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(model_delta_pr_ccal, zorder=2, positions=[2.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(model_delta_pr_scal, zorder=2, positions=[3.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "\n",
    "ax.text(s='original\\nCMIP5 models', x=2.0, y=3.3, fontsize=fontsize, ha='center', va='bottom', color='firebrick')\n",
    "\n",
    "\n",
    "flierprops = dict(marker='.', markeredgecolor='0.1', markerfacecolor='0.1')\n",
    "medianprops = dict(linewidth=2, color='firebrick')\n",
    "boxprops = dict(linewidth=2, color='0.1')\n",
    "whiskerprops = dict(linewidth=2, color='0.1')\n",
    "capprops = dict(linewidth=2, color='0.1')\n",
    "\n",
    "ax.boxplot(subensembles_delta_pr_ncal, zorder=2, positions=[5.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(subensembles_delta_pr_ccal, zorder=2, positions=[6.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(subensembles_delta_pr_scal, zorder=2, positions=[7.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "\n",
    "ax.text(s='all subensembles\\n(N='+str(N_ens)+')', x=6.0, y=3.3, fontsize=fontsize, ha='center', va='bottom', color='firebrick')\n",
    "\n",
    "flierprops = dict(marker='.', markeredgecolor='0.1', markerfacecolor='0.1')\n",
    "medianprops = dict(linewidth=2, color='firebrick')\n",
    "boxprops = dict(linewidth=2, color='0.1')\n",
    "whiskerprops = dict(linewidth=2, color='0.1')\n",
    "capprops = dict(linewidth=2, color='0.1')\n",
    "\n",
    "ax.boxplot(optimal_subensembles_delta_pr_ncal, zorder=2, positions=[9.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(optimal_subensembles_delta_pr_ccal, zorder=2, positions=[10.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(optimal_subensembles_delta_pr_scal, zorder=2, positions=[11.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "\n",
    "# calculate percent change in range\n",
    "ncal_fraction = (optimal_subensembles_delta_pr_ncal.max()-optimal_subensembles_delta_pr_ncal.min())/(model_delta_pr_ncal.max()-model_delta_pr_ncal.min())*100.\n",
    "ccal_fraction = (optimal_subensembles_delta_pr_ccal.max()-optimal_subensembles_delta_pr_ccal.min())/(model_delta_pr_ccal.max()-model_delta_pr_ccal.min())*100.\n",
    "scal_fraction = (optimal_subensembles_delta_pr_scal.max()-optimal_subensembles_delta_pr_scal.min())/(model_delta_pr_scal.max()-model_delta_pr_scal.min())*100.\n",
    "\n",
    "ax.text(s='range\\n'+'{:.0f}'.format(ncal_fraction)+'%', x=9.0, y=2.75, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "ax.text(s='range\\n'+'{:.0f}'.format(ccal_fraction)+'%', x=10.0, y=2.75, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "ax.text(s='range\\n'+'{:.0f}'.format(scal_fraction)+'%', x=11.0, y=2.75, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "\n",
    "ax.text(s='Pareto-optimal\\nsubensembles (N='+str(optimal_subensembles.shape[0])+')', x=10.0, y=3.3, fontsize=fontsize, ha='center', va='bottom', color='firebrick')\n",
    "\n",
    "ax.set_xlim(0,12)\n",
    "ax.set_ylim(-1.25,3.25)\n",
    "ax.tick_params(labelsize=fontsize)\n",
    "ax.set_ylabel('precip. change (mm day$^{\\, -1}$)', fontsize=fontsize)\n",
    "ax.set_xticklabels(['north\\nCalif.', 'central\\nCalif.', 'south\\nCalif.', 'north\\nCalif.', 'central\\nCalif.', 'south\\nCalif.', 'north\\nCalif.', 'central\\nCalif.', 'south\\nCalif.'])\n",
    "ax.set_xticks([1,2,3,5,6,7,9,10,11])\n",
    "ax.grid()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "#fig.savefig('box_plots_delta_pr_constrained_by_combo'+str(which_combo)+'.pdf', transparent=True, bbox_inches='tight')\n",
    "#fig.savefig('box_plots_delta_pr_constrained_by_3D_'+DATESTRING+'_whiskers_range_index_LARGE_domains.pdf', transparent=True, bbox_inches='tight')\n",
    "fig.savefig('box_plots_delta_pr_constrained_by_3D_'+DATESTRING+'_whiskers_hilo_index_LARGE_domains.pdf', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now arrange them in ascending or descending order and take the \n",
    "# 10th, 90th percentile of all subensembles and PO subensembles\n",
    "\n",
    "#model_delta_pr_ncal\n",
    "#subensembles_delta_pr_ccal\n",
    "#optimal_subensembles_delta_pr_ccal\n",
    "\n",
    "# for ccal, for now\n",
    "distro = subensembles_delta_pr_ccal\n",
    "lo_idx_ALL = numpy.where(distro==numpy.percentile(distro, lo_perc, interpolation='nearest'))[0][0]\n",
    "hi_idx_ALL = numpy.where(distro==numpy.percentile(distro, hi_perc, interpolation='nearest'))[0][0]\n",
    "median_idx_ALL = numpy.where(distro==numpy.percentile(distro, med_perc, interpolation='nearest'))[0][0]\n",
    "\n",
    "distro = optimal_subensembles_delta_pr_ccal\n",
    "lo_idx_PO = numpy.where(distro==numpy.percentile(distro, lo_perc, interpolation='nearest'))[0][0]\n",
    "hi_idx_PO = numpy.where(distro==numpy.percentile(distro, hi_perc, interpolation='nearest'))[0][0]\n",
    "median_idx_PO = numpy.where(distro==numpy.percentile(distro, med_perc, interpolation='nearest'))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lo_map_ALL = ALL_subensembles_eoc_pr[lo_idx_ALL] - ALL_subensembles_hist_pr[lo_idx_ALL]\n",
    "hi_map_ALL = ALL_subensembles_eoc_pr[hi_idx_ALL] - ALL_subensembles_hist_pr[hi_idx_ALL]\n",
    "median_map_ALL = ALL_subensembles_eoc_pr[median_idx_ALL] - ALL_subensembles_hist_pr[median_idx_ALL]\n",
    "\n",
    "lo_map_PO = all_po_subensembles_eoc[lo_idx_PO] - all_po_subensembles_hist[lo_idx_PO]\n",
    "hi_map_PO = all_po_subensembles_eoc[hi_idx_PO] - all_po_subensembles_hist[hi_idx_PO]\n",
    "median_map_PO = all_po_subensembles_eoc[median_idx_PO] - all_po_subensembles_hist[median_idx_PO]\n",
    "\n",
    "mean_map_ALL = numpy.mean(ALL_subensembles_eoc_pr - ALL_subensembles_hist_pr, axis=0)\n",
    "mean_map_PO = numpy.mean(all_po_subensembles_eoc - all_po_subensembles_hist, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MidpointNormalize(matplotlib.colors.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        matplotlib.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # I'm ignoring masked values and all kinds of edge cases to make a\n",
    "        # simple example...\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return numpy.ma.masked_array(numpy.interp(value, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All subensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "\n",
    "fig = mp.figure(figsize=(3,6))\n",
    "\n",
    "field_list = [lo_map_ALL, median_map_ALL, hi_map_ALL]\n",
    "file_name = 'pareto_optimal_percentiles_for_combo_3d_front.pdf'\n",
    "figure_text = ['(a) '+str(lo_perc)+'th percentile','(b) Median','(c) '+str(hi_perc)+'th percentile']\n",
    "\n",
    "ax_list = []\n",
    "contour_levels = numpy.hstack((numpy.arange(-0.5,0,0.05), numpy.arange(0,1.8,0.1)))\n",
    "for i in range(3):\n",
    "    \n",
    "    ax = fig.add_subplot(3,1,i+1)\n",
    "    \n",
    "    #ax = mp.subplot2grid((9,4), (i*3,0), colspan=3, rowspan=3)\n",
    "    #ax_list.append(ax)\n",
    "\n",
    "    ax.text(s=figure_text[i],x=0.0,y=1.03,ha='left',va='bottom',transform=ax.transAxes,fontsize=fontsize)\n",
    "\n",
    "    map = basemap.Basemap(projection='cea',llcrnrlat=pr_regional_lat_vals.min(),\\\n",
    "                          urcrnrlat=pr_regional_lat_vals.max(),\\\n",
    "                          llcrnrlon=pr_regional_lon_vals.min(),\\\n",
    "                          urcrnrlon=pr_regional_lon_vals.max(),\\\n",
    "                          resolution='l')\n",
    "\n",
    "    map.drawcoastlines(linewidth=1, color='0') # you can specify white as 1.0, black as 0.0, and any gray as fraction\n",
    "    map.drawmapboundary(linewidth=1, color='0')\n",
    "    map.drawstates(linewidth=1, color='0')\n",
    "    map.drawcountries(linewidth=1, color='0')\n",
    "\n",
    "    #parallels = [35.,40.]\n",
    "    #meridians = [235.,240.,245.]\n",
    "    #map.drawparallels(parallels, labels=[0,0,0,0], fontsize=fontsize)\n",
    "    #map.drawmeridians(meridians, labels=[0,0,0,0], fontsize=fontsize)\n",
    "\n",
    "    lons,lats = numpy.meshgrid(pr_regional_lon_vals, pr_regional_lat_vals)\n",
    "    field = field_list[i]\n",
    "    xi,yi = map(lons, lats)\n",
    "    \n",
    "    xi_smooth = scipy.ndimage.zoom(xi,10)\n",
    "    yi_smooth = scipy.ndimage.zoom(yi,10)\n",
    "    field_smooth = scipy.ndimage.zoom(field,10)\n",
    "\n",
    "    pr_map = map.contourf(xi_smooth, \\\n",
    "                          yi_smooth, \\\n",
    "                          field_smooth, \\\n",
    "                          levels=contour_levels, \\\n",
    "                          extend='max', \\\n",
    "                          norm=MidpointNormalize(midpoint=0), \\\n",
    "                          cmap='RdBu')\n",
    "    \n",
    "    pr_map_contourlines = map.contour(xi_smooth, \\\n",
    "                           yi_smooth, \\\n",
    "                           field_smooth, \\\n",
    "                           levels=[-1,-0.5,0,0.5,1.], \\\n",
    "                           linestyles=['-']*2+['--']+['-']*2, \\\n",
    "                           linewidths=[1]*2+[2]+[1]*2, \\\n",
    "                           colors=['0.25']*5)\n",
    "\n",
    "    for c in pr_map.collections:\n",
    "        c.set_edgecolor(\"face\")\n",
    "    \n",
    "    for gp in range(3):\n",
    "        lat_val = ccal_latlon[gp,1]\n",
    "        lon_val = ccal_latlon[gp,0]\n",
    "        x1,y1 = map(lon_val-1.25, lat_val-1.25)\n",
    "        x2,y2 = map(lon_val-1.25, lat_val+1.25)\n",
    "        x3,y3 = map(lon_val+1.25, lat_val+1.25)\n",
    "        x4,y4 = map(lon_val+1.25, lat_val-1.25)\n",
    "        poly = matplotlib.patches.Polygon([(x1,y1),(x2,y2),(x3,y3),(x4,y4)],\\\n",
    "                                          facecolor='None', edgecolor='0.5',\\\n",
    "                                          linewidth=2,linestyle='-',zorder=2)\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.94, 0.1, 0.04, 0.8])\n",
    "#cbar_ax = mp.subplot2grid((9,4), (0,3), rowspan=9, colspan=0.2)\n",
    "cbar = fig.colorbar(pr_map, cax=cbar_ax, orientation='vertical')\n",
    "cbar.set_label('mm day$^{\\,-1}$', fontsize=fontsize)\n",
    "cbar.ax.tick_params(labelsize=fontsize)\n",
    "cbar.set_ticks(numpy.arange(-0.5,1.8,0.2))\n",
    "cbar.solids.set_edgecolor('face')\n",
    "\n",
    "cbar.add_lines(pr_map_contourlines)\n",
    "cbar_ax.get_children()[1].set_linestyles(['--']*2+['-']+['-']*2)\n",
    "cbar_ax.get_children()[1].set_linewidths([2]*5)\n",
    "\n",
    "fig.tight_layout(h_pad=2)\n",
    "fig.savefig('3D_front_'+str(lo_perc)+'_median_'+str(hi_perc)+'_precip_ALL_subensembles_'+DATESTRING+'_index_LARGE_domains.pdf', bbox_inches='tight', transparent=True)\n",
    "#fig.savefig('2D_front_5_median_95_precip_'+str(which_combo)+'.pdf', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pareto-optimal subensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "\n",
    "fig = mp.figure(figsize=(3,6))\n",
    "\n",
    "field_list = [lo_map_PO, median_map_PO, hi_map_PO]\n",
    "file_name = 'pareto_optimal_percentiles_for_combo_3d_front.pdf'\n",
    "figure_text = ['(a) '+str(lo_perc)+'th percentile','(b) Median','(c) '+str(hi_perc)+'th percentile']\n",
    "\n",
    "ax_list = []\n",
    "contour_levels = numpy.hstack((numpy.arange(-0.5,0,0.05), numpy.arange(0,1.8,0.1)))\n",
    "for i in range(3):\n",
    "    \n",
    "    ax = fig.add_subplot(3,1,i+1)\n",
    "    \n",
    "    #ax = mp.subplot2grid((9,4), (i*3,0), colspan=3, rowspan=3)\n",
    "    #ax_list.append(ax)\n",
    "\n",
    "    ax.text(s=figure_text[i],x=0.0,y=1.03,ha='left',va='bottom',transform=ax.transAxes,fontsize=fontsize)\n",
    "\n",
    "    map = basemap.Basemap(projection='cea',llcrnrlat=pr_regional_lat_vals.min(),\\\n",
    "                          urcrnrlat=pr_regional_lat_vals.max(),\\\n",
    "                          llcrnrlon=pr_regional_lon_vals.min(),\\\n",
    "                          urcrnrlon=pr_regional_lon_vals.max(),\\\n",
    "                          resolution='l')\n",
    "\n",
    "    map.drawcoastlines(linewidth=1, color='0') # you can specify white as 1.0, black as 0.0, and any gray as fraction\n",
    "    map.drawmapboundary(linewidth=1, color='0')\n",
    "    map.drawstates(linewidth=1, color='0')\n",
    "    map.drawcountries(linewidth=1, color='0')\n",
    "\n",
    "    #parallels = [35.,40.]\n",
    "    #meridians = [235.,240.,245.]\n",
    "    #map.drawparallels(parallels, labels=[0,0,0,0], fontsize=fontsize)\n",
    "    #map.drawmeridians(meridians, labels=[0,0,0,0], fontsize=fontsize)\n",
    "\n",
    "    lons,lats = numpy.meshgrid(pr_regional_lon_vals, pr_regional_lat_vals)\n",
    "    field = field_list[i]\n",
    "    xi,yi = map(lons, lats)\n",
    "    \n",
    "    xi_smooth = scipy.ndimage.zoom(xi,10)\n",
    "    yi_smooth = scipy.ndimage.zoom(yi,10)\n",
    "    field_smooth = scipy.ndimage.zoom(field,10)\n",
    "\n",
    "    pr_map = map.contourf(xi_smooth, \\\n",
    "                          yi_smooth, \\\n",
    "                          field_smooth, \\\n",
    "                          levels=contour_levels, \\\n",
    "                          extend='max', \\\n",
    "                          norm=MidpointNormalize(midpoint=0), \\\n",
    "                          cmap='RdBu')\n",
    "    \n",
    "    pr_map_contourlines = map.contour(xi_smooth, \\\n",
    "                           yi_smooth, \\\n",
    "                           field_smooth, \\\n",
    "                           levels=[-1,-0.5,0,0.5,1.], \\\n",
    "                           linestyles=['-']*2+['--']+['-']*2, \\\n",
    "                           linewidths=[1]*2+[2]+[1]*2, \\\n",
    "                           colors=['0.25']*5)\n",
    "\n",
    "    for c in pr_map.collections:\n",
    "        c.set_edgecolor(\"face\")\n",
    "    \n",
    "    \n",
    "    for gp in range(3):\n",
    "        lat_val = ccal_latlon[gp,1]\n",
    "        lon_val = ccal_latlon[gp,0]\n",
    "        x1,y1 = map(lon_val-1.25, lat_val-1.25)\n",
    "        x2,y2 = map(lon_val-1.25, lat_val+1.25)\n",
    "        x3,y3 = map(lon_val+1.25, lat_val+1.25)\n",
    "        x4,y4 = map(lon_val+1.25, lat_val-1.25)\n",
    "        poly = matplotlib.patches.Polygon([(x1,y1),(x2,y2),(x3,y3),(x4,y4)],\\\n",
    "                                          facecolor='None', edgecolor='0.5',\\\n",
    "                                          linewidth=2,linestyle='-',zorder=2)\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.94, 0.1, 0.04, 0.8])\n",
    "#cbar_ax = mp.subplot2grid((9,4), (0,3), rowspan=9, colspan=0.2)\n",
    "cbar = fig.colorbar(pr_map, cax=cbar_ax, orientation='vertical')\n",
    "cbar.set_label('mm day$^{\\,-1}$', fontsize=fontsize)\n",
    "cbar.ax.tick_params(labelsize=fontsize)\n",
    "cbar.set_ticks(numpy.arange(-0.5,1.8,0.2))\n",
    "cbar.solids.set_edgecolor('face')\n",
    "\n",
    "cbar.add_lines(pr_map_contourlines)\n",
    "cbar_ax.get_children()[1].set_linestyles(['--']*2+['-']+['-']*2)\n",
    "cbar_ax.get_children()[1].set_linewidths([2]*5)\n",
    "\n",
    "fig.tight_layout(h_pad=2)\n",
    "fig.savefig('3D_front_'+str(lo_perc)+'_median_'+str(hi_perc)+'_precip_'+DATESTRING+'_index_LARGE_domains.pdf', bbox_inches='tight', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "\n",
    "fig = mp.figure(figsize=(7,6))\n",
    "\n",
    "field_list = [lo_map_PO, median_map_PO, hi_map_PO]\n",
    "file_name = 'pareto_optimal_percentiles_for_combo_3d_front.pdf'\n",
    "figure_text = ['(a) '+str(lo_perc)+'th percentile','(b) Median','(c) '+str(hi_perc)+'th percentile']\n",
    "\n",
    "ax_list = []\n",
    "contour_levels = numpy.hstack((numpy.arange(-0.5,0,0.05), numpy.arange(0,1.8,0.1)))\n",
    "for i in range(3):\n",
    "    \n",
    "    ax = mp.subplot2grid((3,3),(0,i),colspan=1,rowspan=1)\n",
    "\n",
    "    ax.text(s=figure_text[i],x=0.0,y=1.03,ha='left',va='bottom',transform=ax.transAxes,fontsize=fontsize)\n",
    "\n",
    "    map = basemap.Basemap(projection='cea',llcrnrlat=pr_regional_lat_vals.min(),\\\n",
    "                          urcrnrlat=pr_regional_lat_vals.max(),\\\n",
    "                          llcrnrlon=pr_regional_lon_vals.min(),\\\n",
    "                          urcrnrlon=pr_regional_lon_vals.max(),\\\n",
    "                          resolution='l')\n",
    "\n",
    "    map.drawcoastlines(linewidth=1, color='0') # you can specify white as 1.0, black as 0.0, and any gray as fraction\n",
    "    map.drawmapboundary(linewidth=1, color='0')\n",
    "    map.drawstates(linewidth=1, color='0')\n",
    "    map.drawcountries(linewidth=1, color='0')\n",
    "\n",
    "    #parallels = [35.,40.]\n",
    "    #meridians = [235.,240.,245.]\n",
    "    #map.drawparallels(parallels, labels=[0,0,0,0], fontsize=fontsize)\n",
    "    #map.drawmeridians(meridians, labels=[0,0,0,0], fontsize=fontsize)\n",
    "\n",
    "    lons,lats = numpy.meshgrid(pr_regional_lon_vals, pr_regional_lat_vals)\n",
    "    field = field_list[i]\n",
    "    xi,yi = map(lons, lats)\n",
    "    \n",
    "    xi_smooth = scipy.ndimage.zoom(xi,10)\n",
    "    yi_smooth = scipy.ndimage.zoom(yi,10)\n",
    "    field_smooth = scipy.ndimage.zoom(field,10)\n",
    "\n",
    "    pr_map = map.contourf(xi_smooth, \\\n",
    "                          yi_smooth, \\\n",
    "                          field_smooth, \\\n",
    "                          levels=contour_levels, \\\n",
    "                          extend='max', \\\n",
    "                          norm=MidpointNormalize(midpoint=0), \\\n",
    "                          cmap='RdBu')\n",
    "    \n",
    "    pr_map_contourlines = map.contour(xi_smooth, \\\n",
    "                           yi_smooth, \\\n",
    "                           field_smooth, \\\n",
    "                           levels=[-1,-0.5,0,0.5,1.], \\\n",
    "                           linestyles=['-']*2+['--']+['-']*2, \\\n",
    "                           linewidths=[1]*2+[2]+[1]*2, \\\n",
    "                           colors=['0.25']*5)\n",
    "\n",
    "    for c in pr_map.collections:\n",
    "        c.set_edgecolor(\"face\")\n",
    "    \n",
    "    if i==1:\n",
    "    #if i==0:\n",
    "        for gp in range(3):\n",
    "            lat_val = ncal_latlon[gp,1]\n",
    "            lon_val = ncal_latlon[gp,0]\n",
    "            x1,y1 = map(lon_val-1.25, lat_val-1.25)\n",
    "            x2,y2 = map(lon_val-1.25, lat_val+1.25)\n",
    "            x3,y3 = map(lon_val+1.25, lat_val+1.25)\n",
    "            x4,y4 = map(lon_val+1.25, lat_val-1.25)\n",
    "            poly = matplotlib.patches.Polygon([(x1,y1),(x2,y2),(x3,y3),(x4,y4)],\\\n",
    "                                              facecolor='None', edgecolor='0.25',\\\n",
    "                                              linewidth=2,linestyle=':',zorder=2)\n",
    "            ax.add_patch(poly)   \n",
    "    #elif i==2:\n",
    "        for gp in range(3):\n",
    "            lat_val = scal_latlon[gp,1]\n",
    "            lon_val = scal_latlon[gp,0]\n",
    "            x1,y1 = map(lon_val-1.25, lat_val-1.25)\n",
    "            x2,y2 = map(lon_val-1.25, lat_val+1.25)\n",
    "            x3,y3 = map(lon_val+1.25, lat_val+1.25)\n",
    "            x4,y4 = map(lon_val+1.25, lat_val-1.25)\n",
    "            poly = matplotlib.patches.Polygon([(x1,y1),(x2,y2),(x3,y3),(x4,y4)],\\\n",
    "                                              facecolor='None', edgecolor='0.25',\\\n",
    "                                              linewidth=2,linestyle=':',zorder=2)\n",
    "            ax.add_patch(poly)\n",
    "        #elif i==1:\n",
    "        for gp in range(3):\n",
    "            lat_val = ccal_latlon[gp,1]\n",
    "            lon_val = ccal_latlon[gp,0]\n",
    "            x1,y1 = map(lon_val-1.25, lat_val-1.25)\n",
    "            x2,y2 = map(lon_val-1.25, lat_val+1.25)\n",
    "            x3,y3 = map(lon_val+1.25, lat_val+1.25)\n",
    "            x4,y4 = map(lon_val+1.25, lat_val-1.25)\n",
    "            poly = matplotlib.patches.Polygon([(x1,y1),(x2,y2),(x3,y3),(x4,y4)],\\\n",
    "                                              facecolor='None', edgecolor='firebrick',\\\n",
    "                                              linewidth=2,linestyle='-',zorder=2)\n",
    "            ax.add_patch(poly) \n",
    "\n",
    "\n",
    "ax = mp.subplot2grid((3,3),(1,0),colspan=3,rowspan=1)\n",
    "\n",
    "ax.axhline(0,lw=2,c='0.5', zorder=1)\n",
    "\n",
    "boxplot_width=[0.5]\n",
    "#ax.scatter([1]*N_ens, subensembles_delta_pr_ncal, zorder=2)\n",
    "#ax.scatter([2]*N_ens, subensembles_delta_pr_ccal, zorder=2)\n",
    "#ax.scatter([3]*N_ens, subensembles_delta_pr_scal, zorder=2)\n",
    "\n",
    "flierprops = dict(marker='.', markeredgecolor='0.1', markerfacecolor='0.1')\n",
    "medianprops = dict(linewidth=2, color='firebrick')\n",
    "boxprops = dict(linewidth=2, color='0.1')\n",
    "whiskerprops = dict(linewidth=2, color='0.1')\n",
    "capprops = dict(linewidth=2, color='0.1')\n",
    "\n",
    "whisk_range = [lo_perc, hi_perc]\n",
    "#whisk_range = 'range'\n",
    "\n",
    "ax.boxplot(model_delta_pr_ncal, zorder=2, positions=[1.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(model_delta_pr_ccal, zorder=2, positions=[2.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(model_delta_pr_scal, zorder=2, positions=[3.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "\n",
    "ax.text(s='original\\nCMIP5 models', x=2.0, y=-1.8, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "\n",
    "ax.text(s='(d)',x=0.0,y=1.03,ha='left',va='bottom',transform=ax.transAxes,fontsize=fontsize)\n",
    "\n",
    "flierprops = dict(marker='.', markeredgecolor='0.1', markerfacecolor='0.1')\n",
    "medianprops = dict(linewidth=2, color='firebrick')\n",
    "boxprops = dict(linewidth=2, color='0.1')\n",
    "whiskerprops = dict(linewidth=2, color='0.1')\n",
    "capprops = dict(linewidth=2, color='0.1')\n",
    "\n",
    "ax.boxplot(subensembles_delta_pr_ncal, zorder=2, positions=[5.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(subensembles_delta_pr_ccal, zorder=2, positions=[6.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(subensembles_delta_pr_scal, zorder=2, positions=[7.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "\n",
    "ax.text(s='all subensembles\\n(N='+str(N_ens)+')', x=6.0, y=-1.8, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "\n",
    "flierprops = dict(marker='+', s=1e-5, markeredgecolor='0.1', markerfacecolor='0.1')\n",
    "medianprops = dict(linewidth=2, color='firebrick')\n",
    "boxprops = dict(linewidth=2, color='0.1')\n",
    "whiskerprops = dict(linewidth=2, color='0.1')\n",
    "capprops = dict(linewidth=2, color='0.1')\n",
    "\n",
    "ax.boxplot(optimal_subensembles_delta_pr_ncal, zorder=2, positions=[9.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(optimal_subensembles_delta_pr_ccal, zorder=2, positions=[10.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "ax.boxplot(optimal_subensembles_delta_pr_scal, zorder=2, positions=[11.0], whis=whisk_range, showfliers=False, widths=boxplot_width, flierprops=flierprops, medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops)\n",
    "\n",
    "#ax.axvline(x=9.0, ymin=optimal_subensembles_delta_pr_ncal.min(), ymax=optimal_subensembles_delta_pr_ncal.max(), lw=5, c='0.5', zorder=0)\n",
    "#ax.axvline(x=10.0, ymin=optimal_subensembles_delta_pr_ccal.min(), ymax=optimal_subensembles_delta_pr_ncal.max(), lw=5, c='0.5', zorder=0)\n",
    "#ax.axvline(x=11.0, ymin=optimal_subensembles_delta_pr_scal.min(), ymax=optimal_subensembles_delta_pr_ncal.max(), lw=5, c='0.5', zorder=0)\n",
    "\n",
    "# calculate percent change in range\n",
    "ncal_fraction = (optimal_subensembles_delta_pr_ncal.max()-optimal_subensembles_delta_pr_ncal.min())/(model_delta_pr_ncal.max()-model_delta_pr_ncal.min())*100.\n",
    "ccal_fraction = (optimal_subensembles_delta_pr_ccal.max()-optimal_subensembles_delta_pr_ccal.min())/(model_delta_pr_ccal.max()-model_delta_pr_ccal.min())*100.\n",
    "scal_fraction = (optimal_subensembles_delta_pr_scal.max()-optimal_subensembles_delta_pr_scal.min())/(model_delta_pr_scal.max()-model_delta_pr_scal.min())*100.\n",
    "\n",
    "ax.text(s='range\\n'+'{:.0f}'.format(ncal_fraction)+'%', x=9.0, y=-0.25, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "ax.text(s='range\\n'+'{:.0f}'.format(ccal_fraction)+'%', x=10.0, y=-0.25, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "ax.text(s='range\\n'+'{:.0f}'.format(scal_fraction)+'%', x=11.0, y=-0.25, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "\n",
    "ax.text(s='Pareto-optimal\\nsubensembles (N='+str(optimal_subensembles.shape[0])+')', x=10.0, y=-1.8, fontsize=fontsize, ha='center', va='top', color='firebrick')\n",
    "\n",
    "ax.set_xlim(0,12)\n",
    "ax.set_ylim(-1,2)\n",
    "#ax.set_ylim(-2,3)\n",
    "ax.tick_params(labelsize=fontsize)\n",
    "ax.set_ylabel('precip. change\\n(mm day$^{\\, -1}$)', fontsize=fontsize)\n",
    "ax.set_xticklabels(['north\\nCalif.', 'central\\nCalif.', 'south\\nCalif.', 'north\\nCalif.', 'central\\nCalif.', 'south\\nCalif.', 'north\\nCalif.', 'central\\nCalif.', 'south\\nCalif.'])\n",
    "ax.set_xticks([1,2,3,5,6,7,9,10,11])\n",
    "ax.grid()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "fig.tight_layout(h_pad=-2.5, w_pad=1)\n",
    "\n",
    "#fig.savefig('box_plots_delta_pr_constrained_by_combo'+str(which_combo)+'.pdf', transparent=True, bbox_inches='tight')\n",
    "fig.savefig('panel_maps_and_box_plots_'+DATESTRING+'_whiskers_hilo_LARGE_index_domains_alternate.pdf', transparent=True, bbox_inches='tight')\n",
    "\n",
    "#fig.savefig('3D_front_'+str(lo_perc)+'_median_'+str(hi_perc)+'_precip_'+DATESTRING+'_index_domains_NOT_RMSE.pdf', bbox_inches='tight', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving npy stuff\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
