{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as mp\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.cm as cm\n",
    "import mpl_toolkits.mplot3d\n",
    "import matplotlib\n",
    "import scipy.ndimage\n",
    "import datetime\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import numpy.random\n",
    "import scipy.stats\n",
    "import os\n",
    "\n",
    "mp.rcParams.update({'mathtext.default': 'regular'})\n",
    "\n",
    "from mpl_toolkits import basemap\n",
    "import mpl_toolkits.axes_grid1\n",
    "\n",
    "degree_sign = u'\\u00B0'\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = numpy.array(( \\\n",
    "'ACCESS1-0', \\\n",
    "'ACCESS1-3', \\\n",
    "'bcc-csm1-1-m', \\\n",
    "'bcc-csm1-1', \\\n",
    "'BNU-ESM', \\\n",
    "'CanESM2', \\\n",
    "'CCSM4', \\\n",
    "'CESM1-BGC', \\\n",
    "'CESM1-CAM5', \\\n",
    "'CMCC-CESM', \\\n",
    "'CMCC-CM', \\\n",
    "'CMCC-CMS', \\\n",
    "'CNRM-CM5', \\\n",
    "'CSIRO-Mk3-6-0', \\\n",
    "'EC-EARTH', \\\n",
    "'FGOALS-g2', \\\n",
    "'GFDL-CM3', \\\n",
    "'GFDL-ESM2G', \\\n",
    "'GFDL-ESM2M', \\\n",
    "'GISS-E2-H', \\\n",
    "'GISS-E2-R', \\\n",
    "'HadGEM2-AO', \\\n",
    "'HadGEM2-CC', \\\n",
    "'HadGEM2-ES', \\\n",
    "'inmcm4', \\\n",
    "'IPSL-CM5A-LR', \\\n",
    "'IPSL-CM5A-MR', \\\n",
    "'IPSL-CM5B-LR', \\\n",
    "'MIROC5', \\\n",
    "'MIROC-ESM-CHEM', \\\n",
    "'MIROC-ESM', \\\n",
    "'MPI-ESM-LR', \\\n",
    "'MPI-ESM-MR', \\\n",
    "'MRI-CGCM3', \\\n",
    "'NorESM1-ME', \\\n",
    "'NorESM1-M' ))\n",
    "nmods = len(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_lat_lo, pr_lat_hi, pr_lon_lo, pr_lon_hi = 30., 45., 232.5, 248; region = 'CA'\n",
    "ts_lat_lo, ts_lat_hi, ts_lon_lo, ts_lon_hi = -30., 10., 155., 270.; region = 'tropacific'\n",
    "ua_lat_lo, ua_lat_hi, ua_lon_lo, ua_lon_hi = 20., 50., 170., 250.; region = 'midlatpacific'\n",
    "\n",
    "season='djf'; SEASON='DJF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN TS DATASET\n",
    "ncfile = Dataset('../../../DATA/cmip5_data/ts/djf/NorESM1-M_ts_1980-10_climatology_djf.nc', 'r', format='NETCDF4')\n",
    "\n",
    "ts_data_orig = ncfile.variables['ts'][:]\n",
    "ts_lat = ncfile.variables['lat'][:]\n",
    "ts_lon = ncfile.variables['lon'][:]\n",
    "\n",
    "# pull out lat/lon indices\n",
    "ts_lat_inds = numpy.where((ts_lat>=ts_lat_lo) & (ts_lat<=ts_lat_hi))[0]\n",
    "ts_lon_inds = numpy.where((ts_lon>=ts_lon_lo) & (ts_lon<=ts_lon_hi))[0]\n",
    "ts_regional_lat_vals = ts_lat[ts_lat_inds[0]:(ts_lat_inds[-1]+1)]\n",
    "ts_regional_lon_vals = ts_lon[ts_lon_inds[0]:(ts_lon_inds[-1]+1)]    \n",
    "\n",
    "ts_data = ts_data_orig[ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "ts_regional_nlat, ts_regional_nlon = ts_data.shape\n",
    "global_nlat, global_nlon = ts_data_orig.shape[0:2]\n",
    "global_lat_vals = ts_lat[:]\n",
    "global_lon_vals = ts_lon[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN TS OBSERVATIONS\n",
    "ncfile = Dataset('../../../DATA/obs_data/ERSSTv4/obs_ERSSTv4_72x144_SST_1980-2010_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "obs_field_ts = ncfile.variables['sst'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]+273.15\n",
    "\n",
    "# OPEN PR OBSERVATIONS\n",
    "ncfile = Dataset('../../../DATA/obs_data/pr_gpcp/obs_GPCP_72x144_PRECT_1980-2010_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "pr_lat = ncfile.variables['lat'][:]\n",
    "pr_lon = ncfile.variables['lon'][:]\n",
    "pr_lat_inds = numpy.where((pr_lat>=pr_lat_lo) & (pr_lat<=pr_lat_hi))[0]\n",
    "pr_lon_inds = numpy.where((pr_lon>=pr_lon_lo) & (pr_lon<=pr_lon_hi))[0]\n",
    "obs_field_pr = ncfile.variables['PRECT'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "pr_regional_nlat, pr_regional_nlon = obs_field_pr.shape\n",
    "\n",
    "pr_regional_lat_vals = pr_lat[pr_lat_inds[0]:(pr_lat_inds[-1]+1)]\n",
    "pr_regional_lon_vals = pr_lon[pr_lon_inds[0]:(pr_lon_inds[-1]+1)] \n",
    "\n",
    "# OPEN UA OBSERVATIONS\n",
    "ncfile = Dataset('../../../DATA/obs_data/u200_MERRA/obs_MERRA_2.5x2.5_u200_1980-10_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "ua_lat = ncfile.variables['lat'][:]\n",
    "ua_lon = ncfile.variables['lon'][:]\n",
    "ua_lat_inds = numpy.where((ua_lat>=ua_lat_lo) & (ua_lat<=ua_lat_hi))[0]\n",
    "ua_lon_inds = numpy.where((ua_lon>=ua_lon_lo) & (ua_lon<=ua_lon_hi))[0]\n",
    "obs_field_ua = ncfile.variables['u200'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]\n",
    "ua_regional_nlat, ua_regional_nlon = obs_field_ua.shape\n",
    "\n",
    "ua_regional_lat_vals = ua_lat[ua_lat_inds[0]:(ua_lat_inds[-1]+1)]\n",
    "ua_regional_lon_vals = ua_lon[ua_lon_inds[0]:(ua_lon_inds[-1]+1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data\n",
    "model_data_hist_pr = numpy.zeros((len(model_names), pr_regional_nlat, pr_regional_nlon))\n",
    "model_data_eoc_pr = numpy.zeros((len(model_names), pr_regional_nlat, pr_regional_nlon))\n",
    "model_data_hist_pr_LENS = numpy.zeros((40, pr_regional_nlat, pr_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('../../../DATA/cmip5_data/pr/'+season+'/'+modelname+'_pr_1980-10_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_pr[i,:,:] = ncfile.variables['pr'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "    # OPEN FUTURE CHANGE FIELDS\n",
    "    ncfile = Dataset('../../../DATA/cmip5_data/pr/'+season+'/'+modelname+'_pr_2070-99_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_eoc_pr[i,:,:] = ncfile.variables['pr'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "\n",
    "LENS_names = ['{:02d}'.format(i) for i in range(1,36)] + ['{:03d}'.format(i) for i in range(101,106)]\n",
    "for i in range(len(LENS_names)): # 40\n",
    "    member_name = LENS_names[i]\n",
    "    # get convective precipitation\n",
    "    ncfile = Dataset('../../../DATA/lens_data/PRECC/'+season+'/'+member_name + '_PRECC_1980-10_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    precc_temp = ncfile.variables['PRECC'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    # get large-scale precipitation\n",
    "    ncfile = Dataset('../../../DATA/lens_data/PRECL/'+season+'/'+member_name + '_PRECL_1980-10_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    precl_temp = ncfile.variables['PRECL'][pr_lat_inds[0]:(pr_lat_inds[-1]+1), pr_lon_inds[0]:(pr_lon_inds[-1]+1)]\n",
    "    # add together\n",
    "    model_data_hist_pr_LENS[i,:,:] = precc_temp + precl_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TS DATA\n",
    "model_data_hist_ts = numpy.zeros((len(model_names), ts_regional_nlat, ts_regional_nlon))\n",
    "model_data_eoc_ts = numpy.zeros((len(model_names), ts_regional_nlat, ts_regional_nlon))\n",
    "model_data_hist_ts_LENS = numpy.zeros((40, ts_regional_nlat, ts_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('../../../DATA/cmip5_data/ts/'+season+'/'+modelname+'_ts_1980-10_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ts[i,:,:] = ncfile.variables['ts'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "    # OPEN FUTURE CHANGE FIELDS\n",
    "    ncfile = Dataset('../../../DATA/cmip5_data/ts/'+season+'/'+modelname+'_ts_2070-99_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_eoc_ts[i,:,:] = ncfile.variables['ts'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "\n",
    "LENS_names = ['{:02d}'.format(i) for i in range(1,36)] + ['{:03d}'.format(i) for i in range(101,106)]\n",
    "for i in range(len(LENS_names)): # 40\n",
    "    member_name = LENS_names[i]\n",
    "    # get convective precipitation\n",
    "    ncfile = Dataset('../../../DATA/lens_data/TS/'+season+'/'+member_name + '_TS_1980-10_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ts_LENS[i,:,:] = ncfile.variables['TS'][ts_lat_inds[0]:(ts_lat_inds[-1]+1), ts_lon_inds[0]:(ts_lon_inds[-1]+1)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT UA200 DATA\n",
    "model_data_hist_ua = numpy.zeros((len(model_names), ua_regional_nlat, ua_regional_nlon))\n",
    "model_data_eoc_ua = numpy.zeros((len(model_names), ua_regional_nlat, ua_regional_nlon))\n",
    "model_data_hist_ua_LENS = numpy.zeros((40, ua_regional_nlat, ua_regional_nlon))\n",
    "\n",
    "for i in range(nmods):\n",
    "    #print(\"opening model\", model_names[i])\n",
    "    modelname = model_names[i]\n",
    "    # OPEN HISTORICAL FIELDS\n",
    "    ncfile = Dataset('../../../DATA/cmip5_data/ua200/'+season+'/'+modelname+'_ua200_1980-10_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ua[i,:,:] = ncfile.variables['ua'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "    # OPEN FUTURE CHANGE FIELDS\n",
    "    ncfile = Dataset('../../../DATA/cmip5_data/ua200/'+season+'/'+modelname+'_ua200_2070-99_climatology_'+season+'.nc', 'r', format='NETCDF4')\n",
    "    model_data_eoc_ua[i,:,:] = ncfile.variables['ua'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]\n",
    "    ncfile.close()\n",
    "\n",
    "LENS_names = ['{:02d}'.format(i) for i in range(1,36)] + ['{:03d}'.format(i) for i in range(101,106)]\n",
    "for i in range(len(LENS_names)): # 40\n",
    "    member_name = LENS_names[i]\n",
    "    # get convective precipitation\n",
    "    ncfile = Dataset('../../../DATA/lens_data/U200/'+season+'/'+member_name + '_U_1980-10_climatology_'+season+'_2.5x2.5regrid.nc', 'r', format='NETCDF4')\n",
    "    model_data_hist_ua_LENS[i,:,:] = ncfile.variables['U'][ua_lat_inds[0]:(ua_lat_inds[-1]+1), ua_lon_inds[0]:(ua_lon_inds[-1]+1)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncal_latlon = numpy.load('../../../DATA/ncal_latlon_array.npy')\n",
    "ccal_latlon = numpy.load('../../../DATA/ccal_latlon_array.npy')\n",
    "scal_latlon = numpy.load('../../../DATA/scal_latlon_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip indices\n",
    "pr_indices_lon_ncal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, ncal_latlon[i,0]))[0][0] for i in range(ncal_latlon.shape[0]) ]\n",
    "pr_indices_lat_ncal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, ncal_latlon[i,1]))[0][0] for i in range(ncal_latlon.shape[0]) ]\n",
    "\n",
    "pr_indices_lon_ccal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, ccal_latlon[i,0]))[0][0] for i in range(ccal_latlon.shape[0]) ]\n",
    "pr_indices_lat_ccal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, ccal_latlon[i,1]))[0][0] for i in range(ccal_latlon.shape[0]) ]\n",
    "\n",
    "pr_indices_lon_scal = [ numpy.where(numpy.in1d(pr_regional_lon_vals, scal_latlon[i,0]))[0][0] for i in range(scal_latlon.shape[0]) ]\n",
    "pr_indices_lat_scal = [ numpy.where(numpy.in1d(pr_regional_lat_vals, scal_latlon[i,1]))[0][0] for i in range(scal_latlon.shape[0]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take all data and ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW TAKE ALL PR DATA AND RAVEL IT\n",
    "# CALCULATE ENSEMBLE MEAN FOR EOC CONVERGENCE\n",
    "model_field_mmem_pr = numpy.mean(model_data_eoc_pr, axis=0)\n",
    "model_field_mmem_ts = numpy.mean(model_data_eoc_ts, axis=0)\n",
    "model_field_mmem_ua = numpy.mean(model_data_eoc_ua, axis=0)\n",
    "#model_field_mmem_tel = numpy.mean(model_data_hist_tel, axis=0)\n",
    "\n",
    "# NOW CALCULATE BIAS AND CONVERGENCE\n",
    "bias_values_pr = numpy.zeros((nmods))\n",
    "convergence_values_pr = numpy.zeros((nmods))\n",
    "\n",
    "bias_values_ts = numpy.zeros((nmods))\n",
    "convergence_values_ts = numpy.zeros((nmods))\n",
    "\n",
    "bias_values_ua = numpy.zeros((nmods))\n",
    "convergence_values_ua = numpy.zeros((nmods))\n",
    "\n",
    "for i in range(nmods):\n",
    "    hist_field_pr = model_data_hist_pr[i,:,:]\n",
    "    eoc_field_pr = model_data_eoc_pr[i,:,:]\n",
    "    \n",
    "    hist_field_ts = model_data_hist_ts[i,:,:]\n",
    "    eoc_field_ts = model_data_eoc_ts[i,:,:]\n",
    "\n",
    "    hist_field_ua = model_data_hist_ua[i,:,:]\n",
    "    eoc_field_ua = model_data_eoc_ua[i,:,:]\n",
    "\n",
    "    bias_values_pr[i] = numpy.sqrt( numpy.mean((hist_field_pr - obs_field_pr)**2.) )\n",
    "    convergence_values_pr[i] = numpy.sqrt( numpy.mean((eoc_field_pr - model_field_mmem_pr)**2.) )\n",
    "    \n",
    "    bias_values_ts[i] = numpy.sqrt( numpy.mean((hist_field_ts - obs_field_ts)**2.) )\n",
    "    convergence_values_ts[i] = numpy.sqrt( numpy.mean((eoc_field_ts - model_field_mmem_ts)**2.) )\n",
    "    \n",
    "    bias_values_ua[i] = numpy.sqrt( numpy.mean((hist_field_ua - obs_field_ua)**2.) )\n",
    "    convergence_values_ua[i] = numpy.sqrt( numpy.mean((eoc_field_ua - model_field_mmem_ua)**2.) )\n",
    "\n",
    "mmem_bias_pr = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_pr, axis=0) - obs_field_pr)**2. ))\n",
    "mmem_bias_ts = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_ts, axis=0) - obs_field_ts)**2. ))\n",
    "mmem_bias_ua = numpy.sqrt( numpy.mean( (numpy.mean(model_data_hist_ua, axis=0) - obs_field_ua)**2. ))\n",
    "\n",
    "bias_values_pr_LENS = numpy.zeros((40))\n",
    "bias_values_ts_LENS = numpy.zeros((40))\n",
    "bias_values_ua_LENS = numpy.zeros((40))\n",
    "\n",
    "for i in range(40):\n",
    "    hist_field_pr = model_data_hist_pr_LENS[i,:,:]\n",
    "    hist_field_ts = model_data_hist_ts_LENS[i,:,:]\n",
    "    hist_field_ua = model_data_hist_ua_LENS[i,:,:]\n",
    "    \n",
    "    bias_values_pr_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_pr - obs_field_pr)**2.) )\n",
    "    bias_values_ts_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_ts - obs_field_ts)**2.) )\n",
    "    bias_values_ua_LENS[i] = numpy.sqrt( numpy.mean( (hist_field_ua - obs_field_ua)**2.) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to be used below\n",
    "dict_pr = {\n",
    "'bias_values_mods':bias_values_pr,\n",
    "'convergence_values_mods':convergence_values_pr,\n",
    "'bias_values_LENS':bias_values_pr_LENS,\n",
    "'mmem_bias':mmem_bias_pr,\n",
    "'nlat':pr_regional_nlat,\n",
    "'nlon':pr_regional_nlon,\n",
    "'lats':pr_regional_lat_vals,\n",
    "'lons':pr_regional_lon_vals,\n",
    "'fields_hist_mods':model_data_hist_pr,\n",
    "'fields_hist_mods_LENS':model_data_hist_pr_LENS,\n",
    "'fields_eoc_mods':model_data_eoc_pr,\n",
    "'obs_field':obs_field_pr,\n",
    "'LENS':True\n",
    "}\n",
    "\n",
    "dict_ts = {\n",
    "'bias_values_mods':bias_values_ts,\n",
    "'convergence_values_mods':convergence_values_ts,\n",
    "'bias_values_LENS':bias_values_ts_LENS,\n",
    "'mmem_bias':mmem_bias_ts,\n",
    "'nlat':ts_regional_nlat,\n",
    "'nlon':ts_regional_nlon,\n",
    "'lats':ts_regional_lat_vals,\n",
    "'lons':ts_regional_lon_vals,\n",
    "'fields_hist_mods':model_data_hist_ts,\n",
    "'fields_hist_mods_LENS':model_data_hist_ts_LENS,\n",
    "'fields_eoc_mods':model_data_eoc_ts,\n",
    "'obs_field':obs_field_ts,\n",
    "'LENS':True\n",
    "}\n",
    "\n",
    "dict_ua = {\n",
    "'bias_values_mods':bias_values_ua,\n",
    "'convergence_values_mods':convergence_values_ua,\n",
    "'bias_values_LENS':bias_values_ua_LENS,\n",
    "'mmem_bias':mmem_bias_ua,\n",
    "'nlat':ua_regional_nlat,\n",
    "'nlon':ua_regional_nlon,\n",
    "'lats':ua_regional_lat_vals,\n",
    "'lons':ua_regional_lon_vals,\n",
    "'fields_hist_mods':model_data_hist_ua,\n",
    "'fields_hist_mods_LENS':model_data_hist_ua_LENS,\n",
    "'fields_eoc_mods':model_data_eoc_ua,\n",
    "'obs_field':obs_field_ua,\n",
    "'LENS':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_set_collect_2d_list = []\n",
    "pareto_set_collect_3d_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate all biases for CMIP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATESTRING = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "dict_x=dict_pr\n",
    "dict_y=dict_ts\n",
    "dict_z=dict_ua\n",
    "\n",
    "N_pareto_loops=5\n",
    "\n",
    "# do N choose k subensembles\n",
    "# for each, calculate ensemble mean\n",
    "\n",
    "k=6\n",
    "\n",
    "model_numbers = numpy.arange(nmods, dtype=numpy.int)\n",
    "model_combinations = list(itertools.combinations(model_numbers, k))\n",
    "random.shuffle(model_combinations)\n",
    "model_combinations = numpy.array(model_combinations, dtype=numpy.int)\n",
    "\n",
    "N_ens = model_combinations.shape[0]\n",
    "model_combinations = model_combinations[0:N_ens,:]\n",
    "\n",
    "subensembles_hist_x = numpy.zeros((N_ens, dict_x['nlat'], dict_x['nlon']))\n",
    "subensembles_hist_y = numpy.zeros((N_ens, dict_y['nlat'], dict_y['nlon']))\n",
    "subensembles_hist_z = numpy.zeros((N_ens, dict_z['nlat'], dict_z['nlon']))\n",
    "\n",
    "for i in range(N_ens):\n",
    "    subensembles_hist_x[i,:,:] = numpy.mean(dict_x['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "    subensembles_hist_y[i,:,:] = numpy.mean(dict_y['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "    subensembles_hist_z[i,:,:] = numpy.mean(dict_z['fields_hist_mods'][model_combinations[i,:],:,:], axis=0)\n",
    "\n",
    "bias_values_subensembles_x = numpy.zeros((N_ens))\n",
    "bias_values_subensembles_y = numpy.zeros((N_ens))\n",
    "bias_values_subensembles_z = numpy.zeros((N_ens))\n",
    "\n",
    "for i in range(N_ens):\n",
    "    hist_field_x = subensembles_hist_x[i,:,:]\n",
    "    hist_field_y = subensembles_hist_y[i,:,:]\n",
    "    hist_field_z = subensembles_hist_z[i,:,:]\n",
    "\n",
    "    bias_values_subensembles_x[i] = numpy.sqrt( numpy.mean( (hist_field_x - dict_x['obs_field'])**2.) )\n",
    "    bias_values_subensembles_y[i] = numpy.sqrt( numpy.mean( (hist_field_y - dict_y['obs_field'])**2.) )\n",
    "    bias_values_subensembles_z[i] = numpy.sqrt( numpy.mean( (hist_field_z - dict_z['obs_field'])**2.) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Pareto information (which_combo in 2D then 3D last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating Pareto front for 2D combo 1\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n",
      "calculating Pareto front for 2D combo 2\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n",
      "calculating Pareto front for 2D combo 3\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n",
      "calculating 3D Pareto front\n",
      "calculating first Pareto front for 3D surface\n",
      "calculating Pareto front 2\n",
      "calculating Pareto front 3\n",
      "calculating Pareto front 4\n",
      "calculating Pareto front 5\n"
     ]
    }
   ],
   "source": [
    "for which_combo in [1,2,3]:\n",
    "    \n",
    "    ##########\n",
    "    # CALCULATING PARETO FRONT INFO\n",
    "    # FIRST PARETO LOOP IS DONE HERE \n",
    "    print('calculating Pareto front for 2D combo '+str(which_combo))\n",
    "\n",
    "    if which_combo==1:\n",
    "        pareto_array = numpy.vstack((bias_values_subensembles_x, bias_values_subensembles_y)).T\n",
    "    elif which_combo==2:\n",
    "        pareto_array = numpy.vstack((bias_values_subensembles_x, bias_values_subensembles_z)).T\n",
    "    elif which_combo==3:\n",
    "        pareto_array = numpy.vstack((bias_values_subensembles_y, bias_values_subensembles_z)).T\n",
    "    numpy.savetxt('data.txt', pareto_array, delimiter=',')\n",
    "    os.system(\"python ../../../pareto.py data.txt --delimiter=',' --output='pareto_set.txt'\")\n",
    "    pareto_set = numpy.loadtxt('pareto_set.txt', delimiter=',')\n",
    "    n_optima = pareto_set.shape[0]\n",
    "    n_col = pareto_set.shape[1]\n",
    "\n",
    "    pareto_set_collect = numpy.empty((0,2))\n",
    "    pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "\n",
    "    if which_combo==1:\n",
    "        col1_orig = numpy.copy(bias_values_subensembles_x)\n",
    "        col2_orig = numpy.copy(bias_values_subensembles_y)\n",
    "        col1 = numpy.copy(bias_values_subensembles_x)\n",
    "        col2 = numpy.copy(bias_values_subensembles_y)\n",
    "    elif which_combo==2:\n",
    "        col1_orig = numpy.copy(bias_values_subensembles_x)\n",
    "        col2_orig = numpy.copy(bias_values_subensembles_z)\n",
    "        col1 = numpy.copy(bias_values_subensembles_x)\n",
    "        col2 = numpy.copy(bias_values_subensembles_z)\n",
    "    elif which_combo==3:\n",
    "        col1_orig = numpy.copy(bias_values_subensembles_y)\n",
    "        col2_orig = numpy.copy(bias_values_subensembles_z)\n",
    "        col1 = numpy.copy(bias_values_subensembles_y)\n",
    "        col2 = numpy.copy(bias_values_subensembles_z)\n",
    "\n",
    "    # EXTRA PARETO FRONTS ARE DONE HERE, AS LONG AS N_pareto_loops>=1\n",
    "    for loop in range(1,N_pareto_loops):\n",
    "        print('calculating Pareto front '+str(loop+1))\n",
    "        # now find indices where this front occurs\n",
    "        set_indices = numpy.zeros((pareto_set.shape[0]), dtype=numpy.int)\n",
    "        for i in range(pareto_set.shape[0]):\n",
    "            set_indices[i] = numpy.where( (col1==pareto_set[i,0])&(col2==pareto_set[i,1]) )[0]\n",
    "        # and then get rid of them\n",
    "        col1[set_indices] = 999.\n",
    "        col2[set_indices] = 999.\n",
    "\n",
    "        pareto_array = numpy.vstack((col1, col2)).T\n",
    "        numpy.savetxt('data.txt', pareto_array, delimiter=',')\n",
    "        os.system(\"python ../../../pareto.py data.txt --delimiter=',' --output='pareto_set.txt'\")\n",
    "        pareto_set = numpy.loadtxt('pareto_set.txt', delimiter=',')\n",
    "\n",
    "        pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "\n",
    "        n_col = pareto_set.shape[1]    \n",
    "        n_optima = pareto_set_collect.shape[0]\n",
    "    \n",
    "    pareto_set_collect_2d_list.append(pareto_set_collect)\n",
    "\n",
    "# PARETO CALCULATIONS IN 3D\n",
    "print('calculating 3D Pareto front')\n",
    "\n",
    "dict_x=dict_pr\n",
    "dict_y=dict_ts\n",
    "dict_z=dict_ua\n",
    "\n",
    "##########\n",
    "# CALCULATING PARETO FRONT INFO\n",
    "# FIRST PARETO LOOP IS DONE HERE\n",
    "print('calculating first Pareto front for 3D surface')\n",
    "pareto_array = numpy.vstack((bias_values_subensembles_x, bias_values_subensembles_y, bias_values_subensembles_z)).T\n",
    "numpy.savetxt('data.txt', pareto_array, delimiter=',')\n",
    "os.system(\"python ../../../pareto.py data.txt --delimiter=',' --output='pareto_set.txt'\")\n",
    "pareto_set = numpy.loadtxt('pareto_set.txt', delimiter=',')\n",
    "\n",
    "# collect indices\n",
    "set_indices = numpy.zeros(pareto_set.shape[0], dtype=numpy.int)\n",
    "for i in range(pareto_set.shape[0]):\n",
    "    #a = numpy.where( (bias_values_subensembles_x==pareto_set[i,0])&(bias_values_subensembles_y==pareto_set[i,1])&(bias_values_subensembles_z==pareto_set[i,2]) )[0][0]\n",
    "    #print(a)\n",
    "    set_indices[i] = numpy.where( (bias_values_subensembles_x==pareto_set[i,0])&(bias_values_subensembles_y==pareto_set[i,1])&(bias_values_subensembles_z==pareto_set[i,2]) )[0][0]\n",
    "\n",
    "pareto_set_sizes_3d=[]\n",
    "n_optima = pareto_set.shape[0]\n",
    "pareto_set_sizes_3d.append(n_optima)\n",
    "n_col = pareto_set.shape[1]\n",
    "\n",
    "pareto_set_collect = numpy.empty((0,3))\n",
    "pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "set_indices_collect = numpy.empty((0))\n",
    "set_indices_collect = numpy.append(set_indices_collect, set_indices)\n",
    "\n",
    "col1_orig = numpy.copy(bias_values_subensembles_x)\n",
    "col2_orig = numpy.copy(bias_values_subensembles_y)\n",
    "col3_orig = numpy.copy(bias_values_subensembles_z)\n",
    "col1 = numpy.copy(bias_values_subensembles_x)\n",
    "col2 = numpy.copy(bias_values_subensembles_y)\n",
    "col3 = numpy.copy(bias_values_subensembles_z)\n",
    "\n",
    "col1[set_indices] = 999.\n",
    "col2[set_indices] = 999.\n",
    "col3[set_indices] = 999.\n",
    "\n",
    "# EXTRA PARETO FRONTS ARE DONE HERE, AS LONG AS N_pareto_loops>=1\n",
    "for loop in range(1,N_pareto_loops):\n",
    "    print('calculating Pareto front '+str(loop+1))\n",
    "    # now find indices where this front occurs\n",
    "\n",
    "    pareto_array = numpy.vstack((col1, col2, col3)).T\n",
    "    numpy.savetxt('data.txt', pareto_array, delimiter=',')\n",
    "    os.system(\"python pareto.py data.txt --delimiter=',' --output='pareto_set.txt'\")\n",
    "    pareto_set = numpy.loadtxt('pareto_set.txt', delimiter=',')\n",
    "\n",
    "    pareto_set_collect = numpy.append(pareto_set_collect, pareto_set, axis=0)\n",
    "\n",
    "    set_indices = numpy.zeros(pareto_set.shape[0], dtype=numpy.int)\n",
    "    for i in range(pareto_set.shape[0]):\n",
    "        set_indices[i] = numpy.where( (col1==pareto_set[i,0])&(col2==pareto_set[i,1])&(col3==pareto_set[i,2]) )[0][0]\n",
    "    set_indices_collect = numpy.append(set_indices_collect, set_indices)\n",
    "\n",
    "    n_col = pareto_set.shape[1]    \n",
    "    n_optima = pareto_set_collect.shape[0]\n",
    "    pareto_set_sizes_3d.append(pareto_set.shape[0])\n",
    "\n",
    "    col1[set_indices] = 999.\n",
    "    col2[set_indices] = 999.\n",
    "    col3[set_indices] = 999.\n",
    "    \n",
    "pareto_set_collect_3d_list.append(pareto_set_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {}\n",
    "\n",
    "save_dict['pareto_set_collect_2d_list'] = pareto_set_collect_2d_list\n",
    "save_dict['pareto_set_collect_3d_list'] = pareto_set_collect_3d_list\n",
    "\n",
    "save_dict['bias_values_subensembles_x'] = bias_values_subensembles_x\n",
    "save_dict['bias_values_subensembles_y'] = bias_values_subensembles_y\n",
    "save_dict['bias_values_subensembles_z'] = bias_values_subensembles_z\n",
    "\n",
    "save_dict['k'] = k\n",
    "save_dict['N_pareto_loops'] = N_pareto_loops\n",
    "\n",
    "save_dict['N_ens'] = N_ens\n",
    "save_dict['model_combinations'] = model_combinations\n",
    "\n",
    "save_dict['dict_x'] = dict_x\n",
    "save_dict['dict_y'] = dict_y\n",
    "save_dict['dict_z'] = dict_z\n",
    "\n",
    "save_dict['pareto_set_sizes_3d'] = pareto_set_sizes_3d\n",
    "save_dict['model_names'] = model_names\n",
    "\n",
    "save_dir = '../../../DATA/subensemble_data/sampling_k_values/'\n",
    "save_filename = 'pareto_front_results_'+DATESTRING+'_NO_LENS_k=6.npy'\n",
    "numpy.save(save_dir + save_filename, save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-19_15:44:49\n"
     ]
    }
   ],
   "source": [
    "print(DATESTRING)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
